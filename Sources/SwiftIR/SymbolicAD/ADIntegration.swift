// ADIntegration.swift
// Phase 13: Automatic Differentiation Integration with Compilation Pipeline
//
// This implements the "Trojan Horse" mechanism - running Swift's AD-generated
// pullbacks with Tracers to build gradient computation graphs.

import Foundation
import _Differentiation

// MARK: - Helper Functions

/// Format a tensor type string correctly handling empty shapes (scalars)
internal func tensorType(shape: [Int], dtype: DType) -> String {
    if shape.isEmpty {
        return "tensor<\(dtype.rawValue)>"
    } else {
        return "tensor<\(shape.map(String.init).joined(separator: "x"))x\(dtype.rawValue)>"
    }
}

// MARK: - Differentiable Tracer

/// A tracer that supports Swift's automatic differentiation
/// This is the "Trojan Horse" - it looks like a number to Swift's AD
/// but actually builds computation graphs
public struct DifferentiableTracer: Differentiable, AdditiveArithmetic, Sendable {
    public typealias TangentVector = DifferentiableTracer

    /// The MLIR value this tracer represents
    public let irValue: String

    /// Shape of the tensor
    public let shape: [Int]

    /// Data type
    public let dtype: DType

    /// The builder accumulating operations
    nonisolated(unsafe) public static var currentBuilder: MLIRBuilder?

    public init(irValue: String, shape: [Int], dtype: DType = .float32) {
        self.irValue = irValue
        self.shape = shape
        self.dtype = dtype
    }

    // MARK: - Differentiable Protocol

    public mutating func move(by offset: DifferentiableTracer) {
        // Create add operation for gradient accumulation
        guard let builder = DifferentiableTracer.currentBuilder else {
            // If no builder, just replace self
            self = offset
            return
        }

        let result = builder.freshSSA()
        let resultType = tensorType(shape: shape, dtype: dtype)

        builder.addOperation(MLIROperation(
            result: result,
            opName: "add",
            operands: [self.irValue, offset.irValue],
            resultType: resultType
        ))

        self = DifferentiableTracer(irValue: result, shape: shape, dtype: dtype)
    }

    // MARK: - AdditiveArithmetic

    public static var zero: DifferentiableTracer {
        // Create a proper zero constant instead of using a placeholder string
        return createConstant(0.0, shape: [], dtype: .float32)
    }

    // MARK: - Broadcasting Support

    /// Broadcast a tensor to a target shape using stablehlo.broadcast_in_dim
    private static func broadcast(_ tensor: DifferentiableTracer, to targetShape: [Int]) -> DifferentiableTracer {
        guard let builder = currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        // If shapes match, no broadcast needed
        if tensor.shape == targetShape {
            return tensor
        }

        // Calculate broadcast dimensions
        // For example, [4] -> [2, 4] means dim 0 maps to dim 1
        var broadcastDimensions: [Int] = []
        let offset = targetShape.count - tensor.shape.count

        for i in 0..<tensor.shape.count {
            broadcastDimensions.append(offset + i)
        }

        let result = builder.freshSSA()
        let inputType = tensorType(shape: tensor.shape, dtype: tensor.dtype)
        let resultType = tensorType(shape: targetShape, dtype: tensor.dtype)

        builder.addOperation(MLIROperation(
            result: result,
            opName: "broadcast_in_dim",
            operands: [tensor.irValue],
            attributes: [
                "broadcast_dimensions": "[\(broadcastDimensions.map(String.init).joined(separator: ", "))]",
                "_input_type": inputType
            ],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: result, shape: targetShape, dtype: tensor.dtype)
    }

    /// Reduce a tensor by summing along specified dimensions
    /// Used to reduce gradients back to original shape after broadcasting
    private static func reduceSum(_ tensor: DifferentiableTracer, to targetShape: [Int]) -> DifferentiableTracer {
        guard let builder = currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        // If shapes match, no reduction needed
        if tensor.shape == targetShape {
            return tensor
        }

        // Find dimensions to reduce
        // When we broadcast [4] to [2, 4], we need to sum over dim 0
        var dimsToReduce: [Int] = []
        let offset = tensor.shape.count - targetShape.count

        // Any leading dimensions from broadcasting need to be reduced
        for i in 0..<offset {
            dimsToReduce.append(i)
        }

        // Check for dimensions that were 1 in target but expanded
        for i in 0..<targetShape.count {
            if targetShape[i] == 1 && tensor.shape[offset + i] > 1 {
                dimsToReduce.append(offset + i)
            }
        }

        if dimsToReduce.isEmpty {
            return tensor
        }

        // Special case: if target shape has dimensions that need to be kept as size 1
        // we need to use reshape after reduce to maintain the shape
        // For now, we compute the intermediate shape after reduction
        var intermediateShape: [Int] = []
        for (i, dim) in tensor.shape.enumerated() {
            if !dimsToReduce.contains(i) {
                intermediateShape.append(dim)
            }
        }

        // If intermediate shape is empty (scalar) but target is not, we need to reshape
        let needsReshape = intermediateShape.isEmpty && !targetShape.isEmpty

        // First create the init constant (scalar zero)
        let initResult = builder.freshSSA()
        let scalarType = "tensor<\(tensor.dtype.rawValue)>"
        builder.addOperation(MLIROperation(
            result: initResult,
            opName: "constant",
            operands: [],
            attributes: ["value": "dense<0.000000e+00> : \(scalarType)"],
            resultType: scalarType
        ))

        let reduceResult = builder.freshSSA()
        let inputType = tensorType(shape: tensor.shape, dtype: tensor.dtype)

        // Result type is the intermediate shape (what reduce actually produces)
        let reduceResultType: String
        if intermediateShape.isEmpty {
            reduceResultType = "tensor<\(tensor.dtype.rawValue)>"
        } else {
            reduceResultType = tensorType(shape: intermediateShape, dtype: tensor.dtype)
        }

        builder.addOperation(MLIROperation(
            result: reduceResult,
            opName: "reduce_sum",
            operands: [tensor.irValue, initResult],
            attributes: [
                "dimensions": "[\(dimsToReduce.map(String.init).joined(separator: ", "))]",
                "_input_type": inputType
            ],
            resultType: reduceResultType
        ))

        // If we need to reshape from scalar to target shape (e.g., [] -> [1])
        if needsReshape {
            let reshapeResult = builder.freshSSA()
            let targetType = tensorType(shape: targetShape, dtype: tensor.dtype)
            builder.addOperation(MLIROperation(
                result: reshapeResult,
                opName: "reshape",
                operands: [reduceResult],
                resultType: targetType
            ))
            return DifferentiableTracer(irValue: reshapeResult, shape: targetShape, dtype: tensor.dtype)
        }

        return DifferentiableTracer(irValue: reduceResult, shape: targetShape, dtype: tensor.dtype)
    }

    /// Compute the broadcast shape for two tensors
    private static func broadcastShape(_ shape1: [Int], _ shape2: [Int]) -> [Int] {
        let maxLen = max(shape1.count, shape2.count)
        var result: [Int] = []

        for i in 0..<maxLen {
            let dim1 = i < maxLen - shape1.count ? 1 : shape1[i - (maxLen - shape1.count)]
            let dim2 = i < maxLen - shape2.count ? 1 : shape2[i - (maxLen - shape2.count)]

            if dim1 == dim2 {
                result.append(dim1)
            } else if dim1 == 1 {
                result.append(dim2)
            } else if dim2 == 1 {
                result.append(dim1)
            } else {
                fatalError("Incompatible shapes for broadcasting: \(shape1) and \(shape2)")
            }
        }

        return result
    }

    /// Prepare two tensors for a binary operation by broadcasting to a common shape
    private static func broadcastForBinaryOp(
        _ lhs: DifferentiableTracer,
        _ rhs: DifferentiableTracer
    ) -> (DifferentiableTracer, DifferentiableTracer, [Int]) {
        let outputShape = broadcastShape(lhs.shape, rhs.shape)
        let broadcastedLhs = broadcast(lhs, to: outputShape)
        let broadcastedRhs = broadcast(rhs, to: outputShape)
        return (broadcastedLhs, broadcastedRhs, outputShape)
    }

    @differentiable(reverse)
    public static func + (lhs: DifferentiableTracer, rhs: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        // Broadcast to common shape
        let (broadcastedLhs, broadcastedRhs, outputShape) = broadcastForBinaryOp(lhs, rhs)

        let result = builder.freshSSA()
        let resultType = tensorType(shape: outputShape, dtype: lhs.dtype)

        builder.addOperation(MLIROperation(
            result: result,
            opName: "add",
            operands: [broadcastedLhs.irValue, broadcastedRhs.irValue],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: result, shape: outputShape, dtype: lhs.dtype)
    }

    @derivative(of: +)
    public static func addVJP(
        _ lhs: DifferentiableTracer,
        _ rhs: DifferentiableTracer
    ) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
        let y = lhs + rhs
        // Capture original shapes for gradient reduction
        let lhsShape = lhs.shape
        let rhsShape = rhs.shape

        func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
            // Reduce gradients back to original shapes if broadcasting occurred
            let dLhs = reduceSum(dy, to: lhsShape)
            let dRhs = reduceSum(dy, to: rhsShape)
            return (dLhs, dRhs)
        }

        return (y, pullback)
    }

    @differentiable(reverse)
    public static func - (lhs: DifferentiableTracer, rhs: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        // Broadcast to common shape
        let (broadcastedLhs, broadcastedRhs, outputShape) = broadcastForBinaryOp(lhs, rhs)

        let result = builder.freshSSA()
        let resultType = tensorType(shape: outputShape, dtype: lhs.dtype)

        builder.addOperation(MLIROperation(
            result: result,
            opName: "subtract",
            operands: [broadcastedLhs.irValue, broadcastedRhs.irValue],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: result, shape: outputShape, dtype: lhs.dtype)
    }

    @derivative(of: -)
    public static func subtractVJP(
        _ lhs: DifferentiableTracer,
        _ rhs: DifferentiableTracer
    ) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
        let y = lhs - rhs
        // Capture original shapes for gradient reduction
        let lhsShape = lhs.shape
        let rhsShape = rhs.shape

        func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
            guard let builder = currentBuilder else {
                fatalError("No active MLIRBuilder")
            }

            // Negate dy for rhs gradient
            let negResult = builder.freshSSA()
            let resultType = tensorType(shape: dy.shape, dtype: dy.dtype)
            builder.addOperation(MLIROperation(
                result: negResult,
                opName: "negate",
                operands: [dy.irValue],
                resultType: resultType
            ))

            let negDy = DifferentiableTracer(irValue: negResult, shape: dy.shape, dtype: dy.dtype)

            // Reduce gradients back to original shapes if broadcasting occurred
            let dLhs = reduceSum(dy, to: lhsShape)
            let dRhs = reduceSum(negDy, to: rhsShape)
            return (dLhs, dRhs)
        }

        return (y, pullback)
    }
}

// MARK: - Differentiable Operations

extension DifferentiableTracer {
    /// Element-wise multiplication
    @differentiable(reverse)
    public static func * (lhs: DifferentiableTracer, rhs: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        // Broadcast to common shape
        let (broadcastedLhs, broadcastedRhs, outputShape) = broadcastForBinaryOp(lhs, rhs)

        let result = builder.freshSSA()
        let resultType = tensorType(shape: outputShape, dtype: lhs.dtype)

        builder.addOperation(MLIROperation(
            result: result,
            opName: "multiply",
            operands: [broadcastedLhs.irValue, broadcastedRhs.irValue],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: result, shape: outputShape, dtype: lhs.dtype)
    }

    @derivative(of: *)
    public static func multiplyVJP(
        _ lhs: DifferentiableTracer,
        _ rhs: DifferentiableTracer
    ) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
        let y = lhs * rhs
        // Capture original shapes for gradient reduction
        let lhsShape = lhs.shape
        let rhsShape = rhs.shape

        func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
            // d/dlhs = dy * rhs
            // d/drhs = dy * lhs
            let dLhsFull = dy * rhs
            let dRhsFull = dy * lhs

            // Reduce gradients back to original shapes if broadcasting occurred
            let dLhs = reduceSum(dLhsFull, to: lhsShape)
            let dRhs = reduceSum(dRhsFull, to: rhsShape)
            return (dLhs, dRhs)
        }

        return (y, pullback)
    }

    /// Element-wise division
    @differentiable(reverse)
    public static func / (lhs: DifferentiableTracer, rhs: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        // Broadcast to common shape
        let (broadcastedLhs, broadcastedRhs, outputShape) = broadcastForBinaryOp(lhs, rhs)

        let result = builder.freshSSA()
        let resultType = tensorType(shape: outputShape, dtype: lhs.dtype)

        builder.addOperation(MLIROperation(
            result: result,
            opName: "divide",
            operands: [broadcastedLhs.irValue, broadcastedRhs.irValue],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: result, shape: outputShape, dtype: lhs.dtype)
    }

    @derivative(of: /)
    public static func divideVJP(
        _ lhs: DifferentiableTracer,
        _ rhs: DifferentiableTracer
    ) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
        let y = lhs / rhs
        // Capture original shapes for gradient reduction
        let lhsShape = lhs.shape
        let rhsShape = rhs.shape

        func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
            // d/dlhs = dy / rhs
            // d/drhs = -dy * lhs / (rhs * rhs)
            let dLhsFull = dy / rhs

            // Negate dy using negate operation instead of zero - dy
            guard let builder = currentBuilder else {
                fatalError("No active MLIRBuilder")
            }
            let negResult = builder.freshSSA()
            let resultType = tensorType(shape: dy.shape, dtype: dy.dtype)
            builder.addOperation(MLIROperation(
                result: negResult,
                opName: "negate",
                operands: [dy.irValue],
                resultType: resultType
            ))
            let negDy = DifferentiableTracer(irValue: negResult, shape: dy.shape, dtype: dy.dtype)

            let dRhsFull = negDy * lhs / (rhs * rhs)

            // Reduce gradients back to original shapes if broadcasting occurred
            let dLhs = reduceSum(dLhsFull, to: lhsShape)
            let dRhs = reduceSum(dRhsFull, to: rhsShape)
            return (dLhs, dRhs)
        }

        return (y, pullback)
    }
}

// MARK: - Custom Derivatives for Operations

/// Differentiable matrix multiplication
@differentiable(reverse)
public func diffMatmul(_ a: DifferentiableTracer, _ b: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let m = a.shape.count >= 2 ? a.shape[a.shape.count - 2] : 1
    let n = b.shape.count >= 2 ? b.shape[b.shape.count - 1] : 1
    let outputShape = [m, n]

    let result = builder.freshSSA()
    let resultType = tensorType(shape: outputShape, dtype: a.dtype)

    // Build the proper type signature for stablehlo.dot
    let lhsType = tensorType(shape: a.shape, dtype: a.dtype)
    let rhsType = tensorType(shape: b.shape, dtype: b.dtype)
    let dotTypes = "(\(lhsType), \(rhsType)) -> \(resultType)"

    builder.addOperation(MLIROperation(
        result: result,
        opName: "dot",
        operands: [a.irValue, b.irValue],
        attributes: ["_dot_types": dotTypes],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: outputShape, dtype: a.dtype)
}

@derivative(of: diffMatmul)
public func diffMatmulVJP(
    _ a: DifferentiableTracer,
    _ b: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
    let y = diffMatmul(a, b)

    func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
        // da = dy @ b^T
        let bT = diffTranspose(b)
        let da = diffMatmul(dy, bT)

        // db = a^T @ dy
        let aT = diffTranspose(a)
        let db = diffMatmul(aT, dy)

        return (da, db)
    }

    return (y, pullback)
}

/// Differentiable transpose
@differentiable(reverse)
public func diffTranspose(_ a: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let outputShape = a.shape.reversed().map { $0 }
    let result = builder.freshSSA()
    let inputType = tensorType(shape: a.shape, dtype: a.dtype)
    let resultType = tensorType(shape: outputShape, dtype: a.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "transpose",
        operands: [a.irValue],
        attributes: [
            "permutation": "[\(Array(0..<a.shape.count).reversed().map(String.init).joined(separator: ", "))]",
            "_input_type": inputType
        ],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: outputShape, dtype: a.dtype)
}

@derivative(of: diffTranspose)
public func diffTransposeVJP(
    _ a: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffTranspose(a)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        return diffTranspose(dy)
    }

    return (y, pullback)
}

/// Differentiable ReLU
@differentiable(reverse)
public func diffRelu(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    // Create zero constant with same shape
    let zero = createConstant(0.0, shape: x.shape, dtype: x.dtype)

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "maximum",
        operands: [x.irValue, zero.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffRelu)
public func diffReluVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffRelu(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // ReLU gradient: dy * (x > 0)
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        // Create zero constant
        let zero = createConstant(0.0, shape: x.shape, dtype: x.dtype)

        // Create comparison: x > 0
        let cmpResult = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: cmpResult,
            opName: "compare",
            operands: [x.irValue, zero.irValue],
            attributes: ["comparison_direction": "GT"],
            resultType: tensorType(shape: x.shape, dtype: .bool)
        ))

        // Select: dy where x > 0, else 0
        let selectResult = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: selectResult,
            opName: "select",
            operands: [cmpResult, dy.irValue, zero.irValue],
            resultType: tensorType(shape: x.shape, dtype: x.dtype)
        ))

        return DifferentiableTracer(irValue: selectResult, shape: x.shape, dtype: x.dtype)
    }

    return (y, pullback)
}

/// Differentiable exp
@differentiable(reverse)
public func diffExp(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "exp",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffExp)
public func diffExpVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffExp(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // exp gradient: dy * exp(x)
        return dy * y
    }

    return (y, pullback)
}

// MARK: - Phase 15: Additional Differentiable Operations

/// Differentiable natural logarithm
@differentiable(reverse)
public func diffLog(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "log",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffLog)
public func diffLogVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffLog(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // log gradient: dy / x
        return dy / x
    }

    return (y, pullback)
}

/// Differentiable square root
@differentiable(reverse)
public func diffSqrt(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "sqrt",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffSqrt)
public func diffSqrtVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffSqrt(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // sqrt gradient: dy / (2 * sqrt(x))
        let two = createConstant(2.0, shape: x.shape, dtype: x.dtype)
        return dy / (two * y)
    }

    return (y, pullback)
}

/// Differentiable negation
@differentiable(reverse)
public func diffNegate(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "negate",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffNegate)
public func diffNegateVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffNegate(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        return diffNegate(dy)
    }

    return (y, pullback)
}

/// Differentiable sigmoid: 1 / (1 + exp(-x))
@differentiable(reverse)
public func diffSigmoid(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "logistic",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffSigmoid)
public func diffSigmoidVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffSigmoid(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // sigmoid gradient: dy * y * (1 - y)
        let one = createConstant(1.0, shape: y.shape, dtype: y.dtype)
        return dy * y * (one - y)
    }

    return (y, pullback)
}

/// Differentiable tanh
@differentiable(reverse)
public func diffTanh(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "tanh",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffTanh)
public func diffTanhVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffTanh(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // tanh gradient: dy * (1 - y^2)
        let one = createConstant(1.0, shape: y.shape, dtype: y.dtype)
        return dy * (one - y * y)
    }

    return (y, pullback)
}

/// Differentiable absolute value
@differentiable(reverse)
public func diffAbs(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "abs",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffAbs)
public func diffAbsVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffAbs(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // abs gradient: dy * sign(x)
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let signResult = builder.freshSSA()
        let resultType = tensorType(shape: x.shape, dtype: x.dtype)
        builder.addOperation(MLIROperation(
            result: signResult,
            opName: "sign",
            operands: [x.irValue],
            resultType: resultType
        ))

        let sign = DifferentiableTracer(irValue: signResult, shape: x.shape, dtype: x.dtype)
        return dy * sign
    }

    return (y, pullback)
}

/// Differentiable sine
@differentiable(reverse)
public func diffSin(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "sine",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffSin)
public func diffSinVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffSin(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // sin gradient: dy * cos(x)
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let cosResult = builder.freshSSA()
        let resultType = tensorType(shape: x.shape, dtype: x.dtype)
        builder.addOperation(MLIROperation(
            result: cosResult,
            opName: "cosine",
            operands: [x.irValue],
            resultType: resultType
        ))

        let cosX = DifferentiableTracer(irValue: cosResult, shape: x.shape, dtype: x.dtype)
        return dy * cosX
    }

    return (y, pullback)
}

/// Differentiable cosine
@differentiable(reverse)
public func diffCos(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "cosine",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffCos)
public func diffCosVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffCos(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // cos gradient: -dy * sin(x)
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let resultType = tensorType(shape: x.shape, dtype: x.dtype)

        let sinResult = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: sinResult,
            opName: "sine",
            operands: [x.irValue],
            resultType: resultType
        ))

        let sinX = DifferentiableTracer(irValue: sinResult, shape: x.shape, dtype: x.dtype)
        let product = dy * sinX

        // Negate the result
        let negResult = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: negResult,
            opName: "negate",
            operands: [product.irValue],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: negResult, shape: x.shape, dtype: x.dtype)
    }

    return (y, pullback)
}

/// Differentiable sum reduction (reduces all elements)
@differentiable(reverse)
public func diffSum(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    // Sum reduces to scalar
    let resultType = "tensor<\(x.dtype.rawValue)>"
    let inputType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "reduce_sum",
        operands: [x.irValue],
        attributes: [
            "dimensions": "[\(Array(0..<x.shape.count).map(String.init).joined(separator: ", "))]",
            "_input_type": inputType
        ],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: [], dtype: x.dtype)
}

@derivative(of: diffSum)
public func diffSumVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffSum(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // sum gradient: broadcast dy to input shape
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let broadcastResult = builder.freshSSA()
        let scalarType = "tensor<\(dy.dtype.rawValue)>"
        let resultType = tensorType(shape: x.shape, dtype: x.dtype)
        builder.addOperation(MLIROperation(
            result: broadcastResult,
            opName: "broadcast_in_dim",
            operands: [dy.irValue],
            attributes: [
                "broadcast_dimensions": "[]",
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: broadcastResult, shape: x.shape, dtype: x.dtype)
    }

    return (y, pullback)
}

/// Differentiable mean (reduces all elements)
@differentiable(reverse)
public func diffMean(_ x: DifferentiableTracer) -> DifferentiableTracer {
    let sum = diffSum(x)
    let shape = withoutDerivative(at: x.shape)
    let dtype = withoutDerivative(at: x.dtype)
    let count = Float(shape.reduce(1, *))
    let countTracer = createConstant(count, shape: [], dtype: dtype)
    return sum / countTracer
}

/// Differentiable max reduction (reduces all elements to scalar)
@differentiable(reverse)
public func diffMax(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    // Max reduces to scalar
    let resultType = "tensor<\(x.dtype.rawValue)>"
    let inputType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "reduce_max",
        operands: [x.irValue],
        attributes: [
            "dimensions": "[\(Array(0..<x.shape.count).map(String.init).joined(separator: ", "))]",
            "_input_type": inputType
        ],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: [], dtype: x.dtype)
}

@derivative(of: diffMax)
public func diffMaxVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffMax(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // max gradient: dy flows to elements that equal the max
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let resultType = tensorType(shape: x.shape, dtype: x.dtype)
        let scalarType = "tensor<\(x.dtype.rawValue)>"

        // Broadcast max value back to input shape
        let broadcastMax = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: broadcastMax,
            opName: "broadcast_in_dim",
            operands: [y.irValue],
            attributes: [
                "broadcast_dimensions": "[]",
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        // Compare x with max to create mask
        let mask = builder.freshSSA()
        let maskType = tensorType(shape: x.shape, dtype: .bool)
        builder.addOperation(MLIROperation(
            result: mask,
            opName: "compare",
            operands: [x.irValue, broadcastMax],
            attributes: [
                "comparison_direction": "EQ"
            ],
            resultType: maskType
        ))

        // Broadcast dy to input shape
        let broadcastDy = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: broadcastDy,
            opName: "broadcast_in_dim",
            operands: [dy.irValue],
            attributes: [
                "broadcast_dimensions": "[]",
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        // Create zero tensor
        let zeroConst = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: zeroConst,
            opName: "constant",
            operands: [],
            attributes: ["value": "dense<0.000000e+00> : \(scalarType)"],
            resultType: scalarType
        ))

        let zeros = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: zeros,
            opName: "broadcast_in_dim",
            operands: [zeroConst],
            attributes: [
                "broadcast_dimensions": "[]",
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        // Select: where mask is true, use dy, else use 0
        let selected = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: selected,
            opName: "select",
            operands: [mask, broadcastDy, zeros],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: selected, shape: x.shape, dtype: x.dtype)
    }

    return (y, pullback)
}

/// Differentiable min reduction (reduces all elements to scalar)
@differentiable(reverse)
public func diffMin(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    // Min reduces to scalar
    let resultType = "tensor<\(x.dtype.rawValue)>"
    let inputType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "reduce_min",
        operands: [x.irValue],
        attributes: [
            "dimensions": "[\(Array(0..<x.shape.count).map(String.init).joined(separator: ", "))]",
            "_input_type": inputType
        ],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: [], dtype: x.dtype)
}

@derivative(of: diffMin)
public func diffMinVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffMin(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // min gradient: dy flows to elements that equal the min
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let resultType = tensorType(shape: x.shape, dtype: x.dtype)
        let scalarType = "tensor<\(x.dtype.rawValue)>"

        // Broadcast min value back to input shape
        let broadcastMin = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: broadcastMin,
            opName: "broadcast_in_dim",
            operands: [y.irValue],
            attributes: [
                "broadcast_dimensions": "[]",
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        // Compare x with min to create mask
        let mask = builder.freshSSA()
        let maskType = tensorType(shape: x.shape, dtype: .bool)
        builder.addOperation(MLIROperation(
            result: mask,
            opName: "compare",
            operands: [x.irValue, broadcastMin],
            attributes: [
                "comparison_direction": "EQ"
            ],
            resultType: maskType
        ))

        // Broadcast dy to input shape
        let broadcastDy = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: broadcastDy,
            opName: "broadcast_in_dim",
            operands: [dy.irValue],
            attributes: [
                "broadcast_dimensions": "[]",
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        // Create zero tensor
        let zeroConst = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: zeroConst,
            opName: "constant",
            operands: [],
            attributes: ["value": "dense<0.000000e+00> : \(scalarType)"],
            resultType: scalarType
        ))

        let zeros = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: zeros,
            opName: "broadcast_in_dim",
            operands: [zeroConst],
            attributes: [
                "broadcast_dimensions": "[]",
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        // Select: where mask is true, use dy, else use 0
        let selected = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: selected,
            opName: "select",
            operands: [mask, broadcastDy, zeros],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: selected, shape: x.shape, dtype: x.dtype)
    }

    return (y, pullback)
}

/// Differentiable slice operation
/// Extracts a contiguous slice from a tensor
@differentiable(reverse)
public func diffSlice(
    _ x: DifferentiableTracer,
    starts: [Int],
    limits: [Int],
    strides: [Int]? = nil
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let actualStrides = strides ?? Array(repeating: 1, count: x.shape.count)

    // Compute output shape
    var outputShape: [Int] = []
    for i in 0..<x.shape.count {
        let size = (limits[i] - starts[i] + actualStrides[i] - 1) / actualStrides[i]
        outputShape.append(size)
    }

    let result = builder.freshSSA()
    let inputType = tensorType(shape: x.shape, dtype: x.dtype)
    let resultType = tensorType(shape: outputShape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "slice",
        operands: [x.irValue],
        attributes: [
            "start_indices": "[\(starts.map(String.init).joined(separator: ", "))]",
            "limit_indices": "[\(limits.map(String.init).joined(separator: ", "))]",
            "strides": "[\(actualStrides.map(String.init).joined(separator: ", "))]",
            "_input_type": inputType
        ],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: outputShape, dtype: x.dtype)
}

@derivative(of: diffSlice)
public func diffSliceVJP(
    _ x: DifferentiableTracer,
    starts: [Int],
    limits: [Int],
    strides: [Int]?
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffSlice(x, starts: starts, limits: limits, strides: strides)
    let originalShape = x.shape
    let actualStrides = strides ?? Array(repeating: 1, count: x.shape.count)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // Slice gradient: pad the gradient back to original shape
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let resultType = tensorType(shape: originalShape, dtype: x.dtype)

        // Use pad operation to put gradient back in correct position
        // low padding = starts, high padding = original_shape - limits
        var lowPadding: [Int] = []
        var highPadding: [Int] = []
        var interiorPadding: [Int] = []

        for i in 0..<originalShape.count {
            lowPadding.append(starts[i])
            highPadding.append(originalShape[i] - limits[i])
            interiorPadding.append(actualStrides[i] - 1)
        }

        // Create zero padding value
        let zeroConst = builder.freshSSA()
        let scalarType = "tensor<\(x.dtype.rawValue)>"
        builder.addOperation(MLIROperation(
            result: zeroConst,
            opName: "constant",
            operands: [],
            attributes: ["value": "dense<0.000000e+00> : \(scalarType)"],
            resultType: scalarType
        ))

        let padResult = builder.freshSSA()
        let dyType = tensorType(shape: dy.shape, dtype: x.dtype)
        builder.addOperation(MLIROperation(
            result: padResult,
            opName: "pad",
            operands: [dy.irValue, zeroConst],
            attributes: [
                "edge_padding_low": "[\(lowPadding.map(String.init).joined(separator: ", "))]",
                "edge_padding_high": "[\(highPadding.map(String.init).joined(separator: ", "))]",
                "interior_padding": "[\(interiorPadding.map(String.init).joined(separator: ", "))]",
                "_input_type": dyType
            ],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: padResult, shape: originalShape, dtype: x.dtype)
    }

    return (y, pullback)
}

/// Concatenate operation
/// Concatenates tensors along a specified dimension
/// Note: Not differentiable due to array input, use for forward pass only
public func diffConcatenate(
    _ tensors: [DifferentiableTracer],
    dimension: Int
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    guard !tensors.isEmpty else {
        fatalError("Cannot concatenate empty tensor list")
    }

    // Compute output shape
    var outputShape = tensors[0].shape
    for i in 1..<tensors.count {
        outputShape[dimension] += tensors[i].shape[dimension]
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: outputShape, dtype: tensors[0].dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "concatenate",
        operands: tensors.map { $0.irValue },
        attributes: ["dimension": "\(dimension)"],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: outputShape, dtype: tensors[0].dtype)
}

// Note: VJP for concatenate would require returning multiple gradients
// which doesn't fit the standard pullback signature. For now, we provide
// the forward operation. Full gradient support would need a different API.

// MARK: - Convolution Operations

/// 2D Convolution operation
/// input: [batch, in_height, in_width, in_channels] (NHWC format)
/// filter: [filter_height, filter_width, in_channels, out_channels]
/// output: [batch, out_height, out_width, out_channels]
@differentiable(reverse)
public func diffConv2D(
    _ input: DifferentiableTracer,
    _ filter: DifferentiableTracer,
    strides: [Int] = [1, 1],
    padding: String = "VALID"  // "VALID" or "SAME"
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    // Input shape: [batch, in_height, in_width, in_channels]
    // Filter shape: [filter_height, filter_width, in_channels, out_channels]
    guard input.shape.count == 4, filter.shape.count == 4 else {
        fatalError("Conv2D requires 4D input and filter tensors")
    }

    let batch = input.shape[0]
    let inHeight = input.shape[1]
    let inWidth = input.shape[2]
    let filterHeight = filter.shape[0]
    let filterWidth = filter.shape[1]
    let outChannels = filter.shape[3]

    // Compute output dimensions
    let outHeight: Int
    let outWidth: Int
    if padding == "SAME" {
        outHeight = (inHeight + strides[0] - 1) / strides[0]
        outWidth = (inWidth + strides[1] - 1) / strides[1]
    } else {  // VALID
        outHeight = (inHeight - filterHeight) / strides[0] + 1
        outWidth = (inWidth - filterWidth) / strides[1] + 1
    }

    let outputShape = [batch, outHeight, outWidth, outChannels]

    let result = builder.freshSSA()
    let inputType = tensorType(shape: input.shape, dtype: input.dtype)
    let filterType = tensorType(shape: filter.shape, dtype: filter.dtype)
    let resultType = tensorType(shape: outputShape, dtype: input.dtype)

    // Compute padding amounts for SAME padding
    var padLow = [0, 0]
    var padHigh = [0, 0]
    if padding == "SAME" {
        let padHeight = max(0, (outHeight - 1) * strides[0] + filterHeight - inHeight)
        let padWidth = max(0, (outWidth - 1) * strides[1] + filterWidth - inWidth)
        padLow = [padHeight / 2, padWidth / 2]
        padHigh = [padHeight - padLow[0], padWidth - padLow[1]]
    }

    builder.addOperation(MLIROperation(
        result: result,
        opName: "convolution",
        operands: [input.irValue, filter.irValue],
        attributes: [
            "window_strides": "[\(strides.map(String.init).joined(separator: ", "))]",
            "padding": "[[\(padLow[0]), \(padHigh[0])], [\(padLow[1]), \(padHigh[1])]]",
            "dimension_numbers": "#stablehlo.conv<[b, 0, 1, f]x[0, 1, i, o]->[b, 0, 1, f]>",
            "_input_type": inputType,
            "_filter_type": filterType
        ],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: outputShape, dtype: input.dtype)
}

@derivative(of: diffConv2D)
public func diffConv2DVJP(
    _ input: DifferentiableTracer,
    _ filter: DifferentiableTracer,
    strides: [Int],
    padding: String
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
    let output = diffConv2D(input, filter, strides: strides, padding: padding)
    let inputShape = input.shape
    let filterShape = filter.shape
    let outputShape = output.shape

    func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let inputGradType = tensorType(shape: inputShape, dtype: input.dtype)
        let dyType = tensorType(shape: dy.shape, dtype: dy.dtype)
        let filterType = tensorType(shape: filterShape, dtype: filter.dtype)
        let inputType = tensorType(shape: inputShape, dtype: input.dtype)

        // Extract dimensions
        let batch = inputShape[0]
        let inHeight = inputShape[1]
        let inWidth = inputShape[2]
        let inChannels = inputShape[3]
        let filterH = filterShape[0]
        let filterW = filterShape[1]
        let outChannels = filterShape[3]
        let outHeight = outputShape[1]
        let outWidth = outputShape[2]

        // ===== Gradient w.r.t. input =====
        // This is a transposed convolution (conv_transpose)
        // We need to: dilate dy by stride, pad with (filter-1), convolve with flipped filter
        //
        // For NHWC format with filter [H, W, I, O]:
        // Input grad = conv(dy, filter_transposed) where filter is transposed on I/O dims
        // Dimension numbers: dy[b,0,1,o] x filter[0,1,o,i] -> [b,0,1,i]

        let inputGradResult = builder.freshSSA()

        // Compute padding needed for transposed convolution
        // For valid padding: pad = filter_size - 1
        // For same padding: more complex calculation
        let padH = filterH - 1
        let padW = filterW - 1

        // For strided convolutions, we need to dilate the gradient
        // lhs_dilation = original stride
        var lhsDilation = strides
        if strides[0] == 1 && strides[1] == 1 {
            // No dilation needed for stride 1
            builder.addOperation(MLIROperation(
                result: inputGradResult,
                opName: "convolution",
                operands: [dy.irValue, filter.irValue],
                attributes: [
                    "window_strides": "[1, 1]",
                    "padding": "[[\(padH), \(padH)], [\(padW), \(padW)]]",
                    // Swap i and o in filter: [0,1,i,o] -> [0,1,o,i]
                    "dimension_numbers": "#stablehlo.conv<[b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f]>",
                    "feature_group_count": "1",
                    "batch_group_count": "1",
                    "_input_type": dyType,
                    "_filter_type": filterType
                ],
                resultType: inputGradType
            ))
        } else {
            // For strided conv, use lhs_dilation
            builder.addOperation(MLIROperation(
                result: inputGradResult,
                opName: "convolution",
                operands: [dy.irValue, filter.irValue],
                attributes: [
                    "window_strides": "[1, 1]",
                    "padding": "[[\(padH), \(padH)], [\(padW), \(padW)]]",
                    "lhs_dilation": "[\(strides[0]), \(strides[1])]",
                    "dimension_numbers": "#stablehlo.conv<[b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f]>",
                    "feature_group_count": "1",
                    "batch_group_count": "1",
                    "_input_type": dyType,
                    "_filter_type": filterType
                ],
                resultType: inputGradType
            ))
        }

        // ===== Gradient w.r.t. filter =====
        // Filter grad = conv(input, dy) with special dimension mapping
        // Input: [B, H, W, I] treated as lhs
        // Dy: [B, outH, outW, O] treated as rhs
        // Output: [filterH, filterW, I, O]
        //
        // We swap batch and features: input[f,0,1,b] x dy[b,0,1,o] -> [0,1,f,o]
        // This means: treat input_channels as batch, batch as features

        let filterGradResult = builder.freshSSA()
        let filterGradType = tensorType(shape: filterShape, dtype: filter.dtype)

        builder.addOperation(MLIROperation(
            result: filterGradResult,
            opName: "convolution",
            operands: [input.irValue, dy.irValue],
            attributes: [
                "window_strides": "[1, 1]",
                "padding": "[[0, 0], [0, 0]]",
                // lhs: [b,0,1,f] -> interpret b as batch, f as feature
                // rhs: [b,0,1,f] -> interpret b as input_feature (i), f as output_feature (o)
                "dimension_numbers": "#stablehlo.conv<[f, 0, 1, b]x[i, 0, 1, o]->[0, 1, b, f]>",
                "feature_group_count": "1",
                "batch_group_count": "1",
                "_input_type": inputType,
                "_filter_type": dyType
            ],
            resultType: filterGradType
        ))

        let inputGrad = DifferentiableTracer(irValue: inputGradResult, shape: inputShape, dtype: input.dtype)
        let filterGrad = DifferentiableTracer(irValue: filterGradResult, shape: filterShape, dtype: filter.dtype)

        return (inputGrad, filterGrad)
    }

    return (output, pullback)
}

/// Differentiable power function
@differentiable(reverse)
public func diffPow(_ x: DifferentiableTracer, _ n: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "power",
        operands: [x.irValue, n.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffPow)
public func diffPowVJP(
    _ x: DifferentiableTracer,
    _ n: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
    let y = diffPow(x, n)

    func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
        // d/dx (x^n) = n * x^(n-1)
        // d/dn (x^n) = x^n * log(x)
        let one = createConstant(1.0, shape: n.shape, dtype: n.dtype)
        let dx = dy * n * diffPow(x, n - one)
        let dn = dy * y * diffLog(x)
        return (dx, dn)
    }

    return (y, pullback)
}

/// Differentiable square (x^2)
@differentiable(reverse)
public func diffSquare(_ x: DifferentiableTracer) -> DifferentiableTracer {
    return x * x
}

// MARK: - Helper Functions

/// Create a constant tensor
public func createConstant(_ value: Float, shape: [Int], dtype: DType) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType: String
    if shape.isEmpty {
        resultType = "tensor<\(dtype.rawValue)>"
    } else {
        resultType = "tensor<\(shape.map(String.init).joined(separator: "x"))x\(dtype.rawValue)>"
    }

    // Format value based on dtype
    let formattedValue: String
    if dtype == .int32 || dtype == .int64 {
        // For integer types, use integer notation
        formattedValue = String(Int(value))
    } else {
        // For float types, ensure it has decimal point for StableHLO
        if value == 0 {
            formattedValue = "0.000000e+00"
        } else {
            // Use scientific notation with explicit decimal
            formattedValue = String(format: "%.6e", value)
        }
    }

    let attrs = withoutDerivative(at: ["value": "dense<\(formattedValue)> : \(resultType)"])
    builder.addOperation(MLIROperation(
        result: result,
        opName: "constant",
        operands: [],
        attributes: attrs,
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: shape, dtype: dtype)
}

// MARK: - Loss Functions

/// Mean Squared Error loss
@differentiable(reverse)
public func diffMSELoss(_ predictions: DifferentiableTracer, _ targets: DifferentiableTracer) -> DifferentiableTracer {
    let diff = predictions - targets
    let squared = diff * diff
    return diffMean(squared)
}

/// Binary Cross-Entropy loss
@differentiable(reverse)
public func diffBCELoss(_ predictions: DifferentiableTracer, _ targets: DifferentiableTracer) -> DifferentiableTracer {
    let shape = withoutDerivative(at: predictions.shape)
    let dtype = withoutDerivative(at: predictions.dtype)
    let one = createConstant(1.0, shape: shape, dtype: dtype)
    let epsilon = createConstant(1e-7, shape: shape, dtype: dtype)

    // BCE = -mean(targets * log(pred + eps) + (1 - targets) * log(1 - pred + eps))
    let term1 = targets * diffLog(predictions + epsilon)
    let term2 = (one - targets) * diffLog(one - predictions + epsilon)
    let sum = term1 + term2
    return diffNegate(diffMean(sum))
}

// MARK: - Comparison and Selection Operations

/// Element-wise less-than-or-equal comparison
/// Returns a boolean tensor where result[i] = (lhs[i] <= rhs[i])
public func diffLessEqual(_ lhs: DifferentiableTracer, _ rhs: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let shape = withoutDerivative(at: lhs.shape)
    let cmpResult = builder.freshSSA()
    let resultType = tensorType(shape: shape, dtype: .bool)

    builder.addOperation(MLIROperation(
        result: cmpResult,
        opName: "compare",
        operands: [lhs.irValue, rhs.irValue],
        attributes: ["comparison_direction": "LE"],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: cmpResult, shape: shape, dtype: .int32)
}

/// Element-wise selection based on condition
/// Returns: where condition is true, select from trueVals, else from falseVals
/// select(condition, trueVals, falseVals)
@differentiable(reverse, wrt: (trueVals, falseVals))
public func diffSelect(
    _ condition: DifferentiableTracer,
    _ trueVals: DifferentiableTracer,
    _ falseVals: DifferentiableTracer
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let shape = withoutDerivative(at: trueVals.shape)
    let dtype = withoutDerivative(at: trueVals.dtype)

    let selectResult = builder.freshSSA()
    let resultType = tensorType(shape: shape, dtype: dtype)

    builder.addOperation(MLIROperation(
        result: selectResult,
        opName: "select",
        operands: [condition.irValue, trueVals.irValue, falseVals.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: selectResult, shape: shape, dtype: dtype)
}

@derivative(of: diffSelect, wrt: (trueVals, falseVals))
public func diffSelectVJP(
    _ condition: DifferentiableTracer,
    _ trueVals: DifferentiableTracer,
    _ falseVals: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
    let y = diffSelect(condition, trueVals, falseVals)

    func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
        // Gradient flows to trueVals where condition is true, to falseVals where false
        // dL/dtrueVals = select(condition, dy, 0)
        // dL/dfalseVals = select(condition, 0, dy)
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let shape = withoutDerivative(at: trueVals.shape)
        let dtype = withoutDerivative(at: trueVals.dtype)
        let zero = createConstant(0.0, shape: shape, dtype: dtype)

        // Gradient for trueVals: dy where condition is true
        let gradTrue = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: gradTrue,
            opName: "select",
            operands: [condition.irValue, dy.irValue, zero.irValue],
            resultType: tensorType(shape: shape, dtype: dtype)
        ))

        // Gradient for falseVals: dy where condition is false
        let gradFalse = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: gradFalse,
            opName: "select",
            operands: [condition.irValue, zero.irValue, dy.irValue],
            resultType: tensorType(shape: shape, dtype: dtype)
        ))

        return (
            DifferentiableTracer(irValue: gradTrue, shape: shape, dtype: dtype),
            DifferentiableTracer(irValue: gradFalse, shape: shape, dtype: dtype)
        )
    }

    return (y, pullback)
}

// MARK: - Loss Functions

/// Huber loss - robust loss function that is less sensitive to outliers than MSE
/// Huber(x, y, delta) = 0.5 * (x - y)^2                  if |x - y| <= delta
///                    = delta * (|x - y| - 0.5 * delta)  otherwise
///
/// - Parameters:
///   - predictions: Predicted values
///   - targets: Target values
///   - delta: Threshold where loss transitions from quadratic to linear (default: 1.0)
/// - Returns: Scalar loss value
@differentiable(reverse)
public func diffHuberLoss(
    _ predictions: DifferentiableTracer,
    _ targets: DifferentiableTracer,
    delta: Double = 1.0
) -> DifferentiableTracer {
    let shape = withoutDerivative(at: predictions.shape)
    let dtype = withoutDerivative(at: predictions.dtype)

    // Compute absolute difference |x - y|
    let diff = predictions - targets
    let absDiff = diffAbs(diff)

    // Create delta constant
    let deltaConst = createConstant(Float(delta), shape: shape, dtype: dtype)
    let halfDeltaSquared = createConstant(Float(0.5 * delta * delta), shape: shape, dtype: dtype)
    let half = createConstant(0.5, shape: shape, dtype: dtype)

    // Compute quadratic part: 0.5 * diff^2
    let quadratic = half * diff * diff

    // Compute linear part: delta * (|diff| - 0.5 * delta)
    let linear = deltaConst * (absDiff - half * deltaConst)

    // Select based on |diff| <= delta
    // Use: where(|diff| <= delta, quadratic, linear)
    let condition = withoutDerivative(at: diffLessEqual(absDiff, deltaConst))
    let huberLoss = diffSelect(condition, quadratic, linear)

    return diffMean(huberLoss)
}

/// KL Divergence loss - measures how one probability distribution diverges from a reference distribution
/// KL(P||Q) = sum(P * log(P / Q))
///
/// - Parameters:
///   - predictions: Predicted probability distribution (should be normalized, e.g., after softmax)
///   - targets: Target probability distribution
/// - Returns: Scalar KL divergence value
@differentiable(reverse)
public func diffKLDivergence(
    _ predictions: DifferentiableTracer,
    _ targets: DifferentiableTracer
) -> DifferentiableTracer {
    let shape = withoutDerivative(at: predictions.shape)
    let dtype = withoutDerivative(at: predictions.dtype)

    // Add small epsilon to avoid log(0)
    let epsilon = createConstant(1e-10, shape: shape, dtype: dtype)
    let predClipped = predictions + epsilon
    let targetClipped = targets + epsilon

    // KL(P||Q) = sum(P * log(P / Q)) = sum(P * (log(P) - log(Q)))
    // Where P = targets (reference distribution), Q = predictions
    let logPred = diffLog(predClipped)
    let logTarget = diffLog(targetClipped)
    let klPerElement = targetClipped * (logTarget - logPred)

    return diffSum(klPerElement)
}

// MARK: - Higher-Order AD Functions

/// Compute the Jacobian matrix of a function
/// Returns a function that computes the Jacobian at a given input
public func jacobian(
    of function: @escaping @differentiable(reverse) (DifferentiableTracer) -> DifferentiableTracer,
    inputShape: [Int],
    outputShape: [Int],
    dtype: DType = .float32
) throws -> GradientCompiledFunction {
    let compiler = GradientCompiler()

    // Compile the function with gradients
    return try compiler.compileWithGradients(
        inputShape: inputShape,
        dtype: dtype,
        function
    )
}

/// Compute the Hessian (second derivative) of a scalar function
/// For f: R^n -> R, returns the n x n Hessian matrix
public func hessian(
    of function: @escaping @differentiable(reverse) (DifferentiableTracer) -> DifferentiableTracer,
    inputShape: [Int],
    dtype: DType = .float32
) throws -> GradientCompiledFunction {
    // Hessian is the Jacobian of the gradient
    // We compile the gradient function, then differentiate it again

    let compiler = GradientCompiler()

    // First, compile with gradients to get the gradient function
    return try compiler.compileWithGradients(
        inputShape: inputShape,
        dtype: dtype
    ) { x in
        // Forward pass
        let (y, pullback) = valueWithPullback(at: x, of: function)

        // Backward pass with unit seed
        let outputShape = withoutDerivative(at: y.shape)
        let seed = createConstant(1.0, shape: outputShape, dtype: dtype)
        let grad = pullback(seed)

        // Return the gradient (which we'll differentiate again)
        return grad
    }
}

/// JVP (Jacobian-Vector Product) - Forward-mode AD
/// Computes: (f(x), df/dx @ v)
public func jvp(
    at x: DifferentiableTracer,
    in tangent: DifferentiableTracer,
    of function: @escaping @differentiable(reverse) (DifferentiableTracer) -> DifferentiableTracer
) -> (value: DifferentiableTracer, differential: DifferentiableTracer) {
    // Implement JVP using VJP + transpose
    // JVP(x, v) = VJP(x, 1)^T @ v

    let (y, pullback) = valueWithPullback(at: x, of: function)

    // For scalar output, JVP is simple
    let seed = createConstant(1.0, shape: y.shape, dtype: y.dtype)
    let grad = pullback(seed)

    // The differential is grad * tangent
    let differential = grad * tangent

    return (y, differential)
}

// MARK: - Gradient Compilation

/// Compiles a function with its gradients using the Trojan Horse mechanism
public class GradientCompiler {
    private let options: CompilationOptions

    public init(options: CompilationOptions = .default) {
        self.options = options
    }

    /// Compile a differentiable function with gradient computation
    public func compileWithGradients(
        inputShape: [Int],
        dtype: DType = .float32,
        _ function: @escaping @differentiable(reverse) (DifferentiableTracer) -> DifferentiableTracer
    ) throws -> GradientCompiledFunction {
        let builder = MLIRBuilder()
        DifferentiableTracer.currentBuilder = builder

        // Create symbolic input
        let inputName = "%arg0"
        let inputType = tensorType(shape: inputShape, dtype: dtype)
        builder.addArgument(name: inputName, type: inputType)

        let input = DifferentiableTracer(irValue: inputName, shape: inputShape, dtype: dtype)

        // Forward pass: Get value and pullback
        let (output, pullback) = valueWithPullback(at: input, of: function)

        // Record forward output
        let forwardOutputSSA = output.irValue

        // Create seed gradient for backward pass
        let seedName = "%seed"
        let seedType: String
        if output.shape.isEmpty {
            seedType = "tensor<\(dtype.rawValue)>"
        } else {
            seedType = tensorType(shape: output.shape, dtype: dtype)
        }
        builder.addArgument(name: seedName, type: seedType)

        let seed = DifferentiableTracer(irValue: seedName, shape: output.shape, dtype: dtype)

        // THE TROJAN HORSE MOMENT: Run pullback with Tracer
        // This executes Swift's AD-generated gradient code
        // but with Tracers that build MLIR operations!
        let gradient = pullback(seed)

        // Set results: [forward_output, gradient]
        builder.setResults([forwardOutputSSA, gradient.irValue])

        // Build MLIR module
        let module = builder.build(functionName: "main")

        DifferentiableTracer.currentBuilder = nil

        // Compile
        let compiler = SwiftIRCompiler(options: options)
        let compiled = try compiler.compile(module)

        return GradientCompiledFunction(
            compiled: compiled,
            inputShape: inputShape,
            outputShape: output.shape
        )
    }

    /// Compile with multiple inputs
    public func compileWithGradients(
        inputSpecs: [(shape: [Int], dtype: DType)],
        _ function: @escaping @differentiable(reverse) (DifferentiableTracer, DifferentiableTracer) -> DifferentiableTracer
    ) throws -> GradientCompiledFunction {
        let builder = MLIRBuilder()
        DifferentiableTracer.currentBuilder = builder

        // Create symbolic inputs
        var inputs: [DifferentiableTracer] = []
        for (i, spec) in inputSpecs.enumerated() {
            let name = "%arg\(i)"
            let type = tensorType(shape: spec.shape, dtype: spec.dtype)
            builder.addArgument(name: name, type: type)
            inputs.append(DifferentiableTracer(irValue: name, shape: spec.shape, dtype: spec.dtype))
        }

        // Forward pass
        let (output, pullback) = valueWithPullback(at: inputs[0], inputs[1], of: function)
        let forwardOutputSSA = output.irValue

        // Create seed gradient
        let seedName = "%seed"
        let seedType: String
        if output.shape.isEmpty {
            seedType = "tensor<\(inputSpecs[0].dtype.rawValue)>"
        } else {
            seedType = tensorType(shape: output.shape, dtype: inputSpecs[0].dtype)
        }
        builder.addArgument(name: seedName, type: seedType)

        let seed = DifferentiableTracer(irValue: seedName, shape: output.shape, dtype: inputSpecs[0].dtype)

        // Trojan Horse: Run pullback with Tracer
        let (grad0, grad1) = pullback(seed)

        // Set results
        builder.setResults([forwardOutputSSA, grad0.irValue, grad1.irValue])

        let module = builder.build(functionName: "main")

        DifferentiableTracer.currentBuilder = nil

        let compiler = SwiftIRCompiler(options: options)
        let compiled = try compiler.compile(module)

        return GradientCompiledFunction(
            compiled: compiled,
            inputShape: inputSpecs[0].shape,
            outputShape: output.shape
        )
    }
}

// MARK: - Gradient Compiled Function

/// A function compiled with gradient computation
public class GradientCompiledFunction {
    public let compiled: CompiledFunction
    public let inputShape: [Int]
    public let outputShape: [Int]

    public init(
        compiled: CompiledFunction,
        inputShape: [Int],
        outputShape: [Int]
    ) {
        self.compiled = compiled
        self.inputShape = inputShape
        self.outputShape = outputShape
    }

    /// Print the generated MLIR for debugging
    public func printMLIR() {
        print("=== Generated MLIR ===")
        print(compiled.mlirSource)
        print("======================")
    }

    /// Get the generated MLIR source
    public var mlirSource: String {
        compiled.mlirSource
    }

    /// Run forward pass only
    public func forward(_ input: [Float]) -> [Float] {
        let result = compiled.run([[Float](input)])
        return result[0]
    }

    /// Run forward and backward pass
    public func forwardWithGradient(_ input: [Float], seed: [Float]) -> (output: [Float], gradient: [Float]) {
        let result = compiled.run([[Float](input), [Float](seed)])
        return (result[0], result[1])
    }

    public var info: String {
        """
        Gradient Compiled Function:
          Input shape: \(inputShape)
          Output shape: \(outputShape)
        \(compiled.info)
        """
    }
}

// MARK: - Phase 16: PJRT Gradient Execution

extension GradientCompiledFunction {
    /// Convert to a real PJRT-backed gradient function
    public func toPJRTGradientFunction(backend: PJRTBackedRuntime.Backend = .cpu) throws -> PJRTGradientFunction {
        let runtime = try PJRTBackedRuntime(backend: backend)
        let executable = try runtime.compile(compiled.mlirSource)
        return PJRTGradientFunction(
            executable: executable,
            inputShape: inputShape,
            outputShape: outputShape
        )
    }

    /// Convert to a real PJRT-backed gradient function with two inputs
    public func toPJRTGradientFunction2(backend: PJRTBackedRuntime.Backend = .cpu, input2Shape: [Int]) throws -> PJRTGradientFunction2 {
        let runtime = try PJRTBackedRuntime(backend: backend)
        let executable = try runtime.compile(compiled.mlirSource)
        return PJRTGradientFunction2(
            executable: executable,
            input1Shape: inputShape,
            input2Shape: input2Shape,
            outputShape: outputShape
        )
    }
}

/// A gradient function that executes on real PJRT hardware
public class PJRTGradientFunction {
    public let executable: PJRTBackedExecutable
    public let inputShape: [Int]
    public let outputShape: [Int]

    public init(executable: PJRTBackedExecutable, inputShape: [Int], outputShape: [Int]) {
        self.executable = executable
        self.inputShape = inputShape
        self.outputShape = outputShape
    }

    /// Run forward pass only on real hardware
    public func forward(_ input: [Float]) throws -> [Float] {
        // Validate input shape
        let expectedSize = inputShape.reduce(1, *)
        guard input.count == expectedSize else {
            throw PJRTExecutionError.bufferError(
                "Input size mismatch: expected \(expectedSize) elements for shape \(inputShape), got \(input.count)")
        }

        let results = try executable.execute([input, [Float](repeating: 0, count: outputShape.reduce(1, *))])
        return results[0]
    }

    /// Run forward and backward pass on real hardware
    public func forwardWithGradient(_ input: [Float], seed: [Float]) throws -> (output: [Float], gradient: [Float]) {
        // Validate input shape
        let expectedInputSize = inputShape.reduce(1, *)
        guard input.count == expectedInputSize else {
            throw PJRTExecutionError.bufferError(
                "Input size mismatch: expected \(expectedInputSize) elements for shape \(inputShape), got \(input.count)")
        }

        // Validate seed shape
        let expectedSeedSize = outputShape.reduce(1, *)
        guard seed.count == expectedSeedSize else {
            throw PJRTExecutionError.bufferError(
                "Seed size mismatch: expected \(expectedSeedSize) elements for shape \(outputShape), got \(seed.count)")
        }

        let results = try executable.execute([input, seed])
        guard results.count >= 2 else {
            throw PJRTExecutionError.executionFailed("Expected 2 outputs but got \(results.count)")
        }
        return (results[0], results[1])
    }

    /// Compute gradient with unit seed (for scalar output functions)
    public func gradient(_ input: [Float]) throws -> [Float] {
        let seed = [Float](repeating: 1.0, count: outputShape.reduce(1, *))
        let (_, grad) = try forwardWithGradient(input, seed: seed)
        return grad
    }

    public var info: String {
        """
        PJRT Gradient Function:
          Input shape: \(inputShape)
          Output shape: \(outputShape)
        \(executable.info)
        """
    }
}

// MARK: - High-Level PJRT Gradient API

/// Compile a differentiable function to PJRT with gradient support
public func compileGradientForPJRT(
    input: TensorSpec,
    backend: PJRTBackedRuntime.Backend = .cpu,
    _ function: @escaping @differentiable(reverse) (DifferentiableTracer) -> DifferentiableTracer
) throws -> PJRTGradientFunction {
    let compiler = GradientCompiler()
    let gradFunc = try compiler.compileWithGradients(
        inputShape: input.shape,
        dtype: input.dtype,
        function
    )
    return try gradFunc.toPJRTGradientFunction(backend: backend)
}

/// Compile a two-input differentiable function to PJRT with gradient support
public func compileGradientForPJRT(
    inputs: (TensorSpec, TensorSpec),
    backend: PJRTBackedRuntime.Backend = .cpu,
    _ function: @escaping @differentiable(reverse) (DifferentiableTracer, DifferentiableTracer) -> DifferentiableTracer
) throws -> PJRTGradientFunction2 {
    let compiler = GradientCompiler()
    let gradFunc = try compiler.compileWithGradients(
        inputSpecs: [(inputs.0.shape, inputs.0.dtype), (inputs.1.shape, inputs.1.dtype)],
        function
    )
    return try gradFunc.toPJRTGradientFunction2(
        backend: backend,
        input2Shape: inputs.1.shape
    )
}

/// A gradient function with two inputs that executes on real PJRT hardware
public class PJRTGradientFunction2 {
    public let executable: PJRTBackedExecutable
    public let input1Shape: [Int]
    public let input2Shape: [Int]
    public let outputShape: [Int]

    public init(executable: PJRTBackedExecutable, input1Shape: [Int], input2Shape: [Int], outputShape: [Int]) {
        self.executable = executable
        self.input1Shape = input1Shape
        self.input2Shape = input2Shape
        self.outputShape = outputShape
    }

    /// Run forward and backward pass on real hardware
    public func forwardWithGradients(
        _ input1: [Float],
        _ input2: [Float],
        seed: [Float]
    ) throws -> (output: [Float], gradient1: [Float], gradient2: [Float]) {
        // Validate input1 shape
        let expectedInput1Size = input1Shape.reduce(1, *)
        guard input1.count == expectedInput1Size else {
            throw PJRTExecutionError.bufferError(
                "Input1 size mismatch: expected \(expectedInput1Size) elements for shape \(input1Shape), got \(input1.count)")
        }

        // Validate input2 shape
        let expectedInput2Size = input2Shape.reduce(1, *)
        guard input2.count == expectedInput2Size else {
            throw PJRTExecutionError.bufferError(
                "Input2 size mismatch: expected \(expectedInput2Size) elements for shape \(input2Shape), got \(input2.count)")
        }

        // Validate seed shape
        let expectedSeedSize = outputShape.reduce(1, *)
        guard seed.count == expectedSeedSize else {
            throw PJRTExecutionError.bufferError(
                "Seed size mismatch: expected \(expectedSeedSize) elements for shape \(outputShape), got \(seed.count)")
        }

        let results = try executable.execute([input1, input2, seed])
        guard results.count >= 3 else {
            throw PJRTExecutionError.executionFailed("Expected 3 outputs but got \(results.count)")
        }
        return (results[0], results[1], results[2])
    }

    /// Compute gradients with unit seed (for scalar output functions)
    public func gradients(_ input1: [Float], _ input2: [Float]) throws -> (gradient1: [Float], gradient2: [Float]) {
        let seed = [Float](repeating: 1.0, count: outputShape.reduce(1, *))
        let (_, grad1, grad2) = try forwardWithGradients(input1, input2, seed: seed)
        return (grad1, grad2)
    }

    public var info: String {
        """
        PJRT Gradient Function (2 inputs):
          Input1 shape: \(input1Shape)
          Input2 shape: \(input2Shape)
          Output shape: \(outputShape)
        \(executable.info)
        """
    }
}

// MARK: - High-Level API

/// Compile a differentiable function with gradients
public func compileWithGradients(
    input: TensorSpec,
    options: CompilationOptions = .default,
    _ function: @escaping @differentiable(reverse) (DifferentiableTracer) -> DifferentiableTracer
) throws -> GradientCompiledFunction {
    let compiler = GradientCompiler(options: options)
    return try compiler.compileWithGradients(
        inputShape: input.shape,
        dtype: input.dtype,
        function
    )
}

/// Compile a two-input differentiable function with gradients
public func compileWithGradients(
    inputs: (TensorSpec, TensorSpec),
    options: CompilationOptions = .default,
    _ function: @escaping @differentiable(reverse) (DifferentiableTracer, DifferentiableTracer) -> DifferentiableTracer
) throws -> GradientCompiledFunction {
    let compiler = GradientCompiler(options: options)
    return try compiler.compileWithGradients(
        inputSpecs: [(inputs.0.shape, inputs.0.dtype), (inputs.1.shape, inputs.1.dtype)],
        function
    )
}

// MARK: - Training Loop Support

/// A compiled training step with forward, backward, and parameter updates
public class CompiledTrainingStep {
    public let gradientFunc: GradientCompiledFunction
    public let learningRate: Float

    public init(gradientFunc: GradientCompiledFunction, learningRate: Float = 0.01) {
        self.gradientFunc = gradientFunc
        self.learningRate = learningRate
    }

    /// Execute one training step
    public func step(input: [Float], seed: [Float], parameters: inout [Float]) {
        let (_, gradient) = gradientFunc.forwardWithGradient(input, seed: seed)

        // Update parameters: p -= lr * grad
        for i in 0..<min(parameters.count, gradient.count) {
            parameters[i] -= learningRate * gradient[i]
        }
    }
}

// MARK: - Phase 15: Optimizers

/// Protocol for optimizers
public protocol Optimizer {
    mutating func update(parameters: inout [Float], gradients: [Float])
}

/// Stochastic Gradient Descent optimizer
public struct SGDOptimizer: Optimizer {
    public let learningRate: Float
    public let momentum: Float
    private var velocity: [Float]?

    public init(learningRate: Float = 0.01, momentum: Float = 0.0) {
        self.learningRate = learningRate
        self.momentum = momentum
        self.velocity = nil
    }

    public mutating func update(parameters: inout [Float], gradients: [Float]) {
        if velocity == nil {
            velocity = [Float](repeating: 0.0, count: parameters.count)
        }

        for i in 0..<parameters.count {
            velocity![i] = momentum * velocity![i] - learningRate * gradients[i]
            parameters[i] += velocity![i]
        }
    }
}

/// Adam optimizer
public struct AdamOptimizer: Optimizer {
    public let learningRate: Float
    public let beta1: Float
    public let beta2: Float
    public let epsilon: Float
    private var m: [Float]?  // First moment
    private var v: [Float]?  // Second moment
    private var t: Int = 0   // Timestep

    public init(
        learningRate: Float = 0.001,
        beta1: Float = 0.9,
        beta2: Float = 0.999,
        epsilon: Float = 1e-8
    ) {
        self.learningRate = learningRate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
    }

    public mutating func update(parameters: inout [Float], gradients: [Float]) {
        t += 1

        if m == nil {
            m = [Float](repeating: 0.0, count: parameters.count)
            v = [Float](repeating: 0.0, count: parameters.count)
        }

        for i in 0..<parameters.count {
            // Update biased first moment estimate
            m![i] = beta1 * m![i] + (1 - beta1) * gradients[i]

            // Update biased second raw moment estimate
            v![i] = beta2 * v![i] + (1 - beta2) * gradients[i] * gradients[i]

            // Compute bias-corrected first moment estimate
            let mHat = m![i] / (1 - pow(beta1, Float(t)))

            // Compute bias-corrected second raw moment estimate
            let vHat = v![i] / (1 - pow(beta2, Float(t)))

            // Update parameters
            parameters[i] -= learningRate * mHat / (sqrt(vHat) + epsilon)
        }
    }
}

/// RMSprop optimizer
public struct RMSpropOptimizer: Optimizer {
    public let learningRate: Float
    public let rho: Float
    public let epsilon: Float
    private var cache: [Float]?

    public init(learningRate: Float = 0.001, rho: Float = 0.9, epsilon: Float = 1e-8) {
        self.learningRate = learningRate
        self.rho = rho
        self.epsilon = epsilon
    }

    public mutating func update(parameters: inout [Float], gradients: [Float]) {
        if cache == nil {
            cache = [Float](repeating: 0.0, count: parameters.count)
        }

        for i in 0..<parameters.count {
            cache![i] = rho * cache![i] + (1 - rho) * gradients[i] * gradients[i]
            parameters[i] -= learningRate * gradients[i] / (sqrt(cache![i]) + epsilon)
        }
    }
}

// MARK: - Training Utilities

/// Training loop configuration
public struct TrainingConfig {
    public let epochs: Int
    public let batchSize: Int
    public let learningRate: Float
    public let printEvery: Int

    public init(
        epochs: Int = 100,
        batchSize: Int = 32,
        learningRate: Float = 0.01,
        printEvery: Int = 10
    ) {
        self.epochs = epochs
        self.batchSize = batchSize
        self.learningRate = learningRate
        self.printEvery = printEvery
    }
}

/// Training result
public struct TrainingResult {
    public let finalLoss: Float
    public let losses: [Float]
    public let epochs: Int

    public var info: String {
        """
        Training Result:
          Epochs: \(epochs)
          Final Loss: \(finalLoss)
          Loss progression: \(losses.prefix(5).map { String(format: "%.4f", $0) }.joined(separator: " -> "))...
        """
    }
}

/// Simple training loop
public func train(
    model: GradientCompiledFunction,
    data: [[Float]],
    targets: [[Float]],
    config: TrainingConfig,
    optimizer: inout some Optimizer
) -> TrainingResult {
    var losses: [Float] = []
    var parameters = data[0]  // Initial parameters

    for epoch in 0..<config.epochs {
        var epochLoss: Float = 0.0

        for (input, target) in zip(data, targets) {
            let (output, gradient) = model.forwardWithGradient(input, seed: target)

            // Compute loss (MSE)
            let loss = zip(output, target).reduce(0.0) { acc, pair in
                let diff = pair.0 - pair.1
                return acc + diff * diff
            } / Float(output.count)

            epochLoss += loss
            optimizer.update(parameters: &parameters, gradients: gradient)
        }

        epochLoss /= Float(data.count)
        losses.append(epochLoss)

        if epoch % config.printEvery == 0 {
            print("Epoch \(epoch): Loss = \(epochLoss)")
        }
    }

    return TrainingResult(
        finalLoss: losses.last ?? 0.0,
        losses: losses,
        epochs: config.epochs
    )
}

// MARK: - Phase 31: Softmax and Cross-Entropy Loss

/// Numerically stable softmax: softmax(x) = exp(x - max(x)) / sum(exp(x - max(x)))
/// Applied along the last axis
@differentiable(reverse)
public func diffSoftmax(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let shape = x.shape
    let dtype = x.dtype
    let resultType = tensorType(shape: shape, dtype: dtype)

    // For numerical stability: subtract max along last axis
    // Step 1: Compute max along last axis
    let maxVal = diffMax(x)

    // Step 2: Broadcast max back to original shape
    let broadcastMax = builder.freshSSA()
    let scalarType = "tensor<\(dtype.rawValue)>"
    let broadcastDims = shape.count == 1 ? "[]" : "[\(Array(0..<(shape.count-1)).map(String.init).joined(separator: ", "))]"

    builder.addOperation(MLIROperation(
        result: broadcastMax,
        opName: "broadcast_in_dim",
        operands: [maxVal.irValue],
        attributes: [
            "broadcast_dimensions": broadcastDims,
            "_input_type": scalarType
        ],
        resultType: resultType
    ))

    let broadcastMaxTracer = DifferentiableTracer(irValue: broadcastMax, shape: shape, dtype: dtype)

    // Step 3: x_shifted = x - max
    let xShifted = x - broadcastMaxTracer

    // Step 4: exp(x_shifted)
    let expX = diffExp(xShifted)

    // Step 5: sum(exp(x_shifted))
    let sumExp = diffSum(expX)

    // Step 6: Broadcast sum back
    let broadcastSum = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: broadcastSum,
        opName: "broadcast_in_dim",
        operands: [sumExp.irValue],
        attributes: [
            "broadcast_dimensions": broadcastDims,
            "_input_type": scalarType
        ],
        resultType: resultType
    ))

    let broadcastSumTracer = DifferentiableTracer(irValue: broadcastSum, shape: shape, dtype: dtype)

    // Step 7: softmax = exp / sum
    return expX / broadcastSumTracer
}

// Softmax VJP: Jacobian-vector product
// d(softmax)/dx[i] = softmax[i] * (1[i==j] - softmax[j])
// For vector y = softmax(x), grad = dy * y - y * sum(dy * y)
@derivative(of: diffSoftmax)
public func diffSoftmaxVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffSoftmax(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let shape = x.shape
        let dtype = x.dtype
        let resultType = tensorType(shape: shape, dtype: dtype)
        let scalarType = "tensor<\(dtype.rawValue)>"

        // grad = y * (dy - sum(dy * y))
        let dyTimesY = dy * y
        let sumDyY = diffSum(dyTimesY)

        // Broadcast sum back
        let broadcastSum = builder.freshSSA()
        let broadcastDims = shape.count == 1 ? "[]" : "[\(Array(0..<(shape.count-1)).map(String.init).joined(separator: ", "))]"

        builder.addOperation(MLIROperation(
            result: broadcastSum,
            opName: "broadcast_in_dim",
            operands: [sumDyY.irValue],
            attributes: [
                "broadcast_dimensions": broadcastDims,
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        let broadcastSumTracer = DifferentiableTracer(irValue: broadcastSum, shape: shape, dtype: dtype)

        return y * (dy - broadcastSumTracer)
    }

    return (y, pullback)
}

/// Cross-entropy loss: -sum(target * log(pred))
/// Assumes pred is already softmax output (probabilities)
@differentiable(reverse)
public func diffCrossEntropy(
    _ predictions: DifferentiableTracer,
    _ targets: DifferentiableTracer
) -> DifferentiableTracer {
    // -sum(targets * log(predictions))
    // Add small epsilon for numerical stability
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let eps = createConstant(1e-7, shape: predictions.shape, dtype: predictions.dtype)
    let logPred = diffLog(predictions + eps)
    let loss = targets * logPred
    let sumLoss = diffSum(loss)

    // Negate
    let result = builder.freshSSA()
    let scalarType = "tensor<\(predictions.dtype.rawValue)>"
    builder.addOperation(MLIROperation(
        result: result,
        opName: "negate",
        operands: [sumLoss.irValue],
        resultType: scalarType
    ))

    return DifferentiableTracer(irValue: result, shape: [], dtype: predictions.dtype)
}

@derivative(of: diffCrossEntropy)
public func diffCrossEntropyVJP(
    _ predictions: DifferentiableTracer,
    _ targets: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
    let loss = diffCrossEntropy(predictions, targets)

    func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let shape = predictions.shape
        let dtype = predictions.dtype
        let resultType = tensorType(shape: shape, dtype: dtype)
        let scalarType = "tensor<\(dtype.rawValue)>"

        // d/dpred (-sum(target * log(pred))) = -target / pred
        let eps = createConstant(1e-7, shape: shape, dtype: dtype)
        let gradPred = targets / (predictions + eps)

        // Broadcast dy to shape
        let broadcastDy = builder.freshSSA()
        let broadcastDims = shape.count == 1 ? "[]" : "[\(Array(0..<(shape.count-1)).map(String.init).joined(separator: ", "))]"

        builder.addOperation(MLIROperation(
            result: broadcastDy,
            opName: "broadcast_in_dim",
            operands: [dy.irValue],
            attributes: [
                "broadcast_dimensions": broadcastDims,
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        let broadcastDyTracer = DifferentiableTracer(irValue: broadcastDy, shape: shape, dtype: dtype)

        // Negate the gradient
        let negGrad = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: negGrad,
            opName: "negate",
            operands: [gradPred.irValue],
            resultType: resultType
        ))

        let negGradTracer = DifferentiableTracer(irValue: negGrad, shape: shape, dtype: dtype)
        let dPred = broadcastDyTracer * negGradTracer

        // Gradient w.r.t. targets is -log(pred), but usually targets are not trained
        let dTargets = createConstant(0.0, shape: shape, dtype: dtype)

        return (dPred, dTargets)
    }

    return (loss, pullback)
}

/// Softmax cross-entropy loss - combined for numerical stability
/// Computes: -sum(targets * log(softmax(logits)))
/// The gradient is simply: softmax(logits) - targets
@differentiable(reverse)
public func diffSoftmaxCrossEntropy(
    _ logits: DifferentiableTracer,
    _ targets: DifferentiableTracer
) -> DifferentiableTracer {
    let probs = diffSoftmax(logits)
    return diffCrossEntropy(probs, targets)
}

@derivative(of: diffSoftmaxCrossEntropy)
public func diffSoftmaxCrossEntropyVJP(
    _ logits: DifferentiableTracer,
    _ targets: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
    let probs = diffSoftmax(logits)
    let loss = diffCrossEntropy(probs, targets)

    func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let shape = logits.shape
        let dtype = logits.dtype
        let resultType = tensorType(shape: shape, dtype: dtype)
        let scalarType = "tensor<\(dtype.rawValue)>"

        // The gradient of softmax cross-entropy is beautifully simple:
        // d/dlogits = probs - targets
        // This is much more stable than computing through softmax and cross-entropy separately

        // Broadcast dy to shape
        let broadcastDy = builder.freshSSA()
        let broadcastDims = shape.count == 1 ? "[]" : "[\(Array(0..<(shape.count-1)).map(String.init).joined(separator: ", "))]"

        builder.addOperation(MLIROperation(
            result: broadcastDy,
            opName: "broadcast_in_dim",
            operands: [dy.irValue],
            attributes: [
                "broadcast_dimensions": broadcastDims,
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        let broadcastDyTracer = DifferentiableTracer(irValue: broadcastDy, shape: shape, dtype: dtype)

        let dLogits = broadcastDyTracer * (probs - targets)
        let dTargets = createConstant(0.0, shape: shape, dtype: dtype)

        return (dLogits, dTargets)
    }

    return (loss, pullback)
}

// MARK: - Phase 30: Additional Element-wise Operations

/// Floor operation - rounds down to nearest integer
/// Note: Not differentiable (gradient is zero almost everywhere)
public func diffFloor(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "floor",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

/// Ceil operation - rounds up to nearest integer
/// Note: Not differentiable (gradient is zero almost everywhere)
public func diffCeil(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "ceil",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

/// Clamp operation - clips values to [min, max] range
@differentiable(reverse)
public func diffClamp(
    _ x: DifferentiableTracer,
    min minVal: DifferentiableTracer,
    max maxVal: DifferentiableTracer
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "clamp",
        operands: [minVal.irValue, x.irValue, maxVal.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffClamp)
public func diffClampVJP(
    _ x: DifferentiableTracer,
    min minVal: DifferentiableTracer,
    max maxVal: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer, DifferentiableTracer)) {
    let y = diffClamp(x, min: minVal, max: maxVal)

    func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer, DifferentiableTracer) {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let resultType = tensorType(shape: x.shape, dtype: x.dtype)
        let maskType = tensorType(shape: x.shape, dtype: .bool)

        // Gradient flows through only where input is not clamped
        // mask_x = (x > min) && (x < max)
        let gtMin = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: gtMin,
            opName: "compare",
            operands: [x.irValue, minVal.irValue],
            attributes: ["comparison_direction": "GT"],
            resultType: maskType
        ))

        let ltMax = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: ltMax,
            opName: "compare",
            operands: [x.irValue, maxVal.irValue],
            attributes: ["comparison_direction": "LT"],
            resultType: maskType
        ))

        let mask = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: mask,
            opName: "and",
            operands: [gtMin, ltMax],
            resultType: maskType
        ))

        // Create zeros
        let zeros = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: zeros,
            opName: "constant",
            operands: [],
            attributes: ["value": "dense<0.000000e+00> : \(resultType)"],
            resultType: resultType
        ))

        let dx = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: dx,
            opName: "select",
            operands: [mask, dy.irValue, zeros],
            resultType: resultType
        ))

        let dxTracer = DifferentiableTracer(irValue: dx, shape: x.shape, dtype: x.dtype)
        let dMin = createConstant(0.0, shape: minVal.shape, dtype: minVal.dtype)
        let dMax = createConstant(0.0, shape: maxVal.shape, dtype: maxVal.dtype)

        return (dxTracer, dMin, dMax)
    }

    return (y, pullback)
}

/// Rsqrt operation - reciprocal square root: 1/sqrt(x)
@differentiable(reverse)
public func diffRsqrt(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let result = builder.freshSSA()
    let resultType = tensorType(shape: x.shape, dtype: x.dtype)

    builder.addOperation(MLIROperation(
        result: result,
        opName: "rsqrt",
        operands: [x.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: x.shape, dtype: x.dtype)
}

@derivative(of: diffRsqrt)
public func diffRsqrtVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffRsqrt(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // d/dx (1/sqrt(x)) = -0.5 * x^(-3/2) = -0.5 * rsqrt(x)^3
        let halfNeg = createConstant(-0.5, shape: x.shape, dtype: x.dtype)
        let yCubed = y * y * y
        return dy * halfNeg * yCubed
    }

    return (y, pullback)
}

// MARK: - Phase 32: Additional Activation Functions

/// Leaky ReLU: f(x) = x if x > 0, else alpha * x
/// Default alpha = 0.01
@differentiable(reverse, wrt: x)
public func diffLeakyReLU(_ x: DifferentiableTracer, alpha: Float = 0.01) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let shape = x.shape
    let dtype = x.dtype
    let resultType = tensorType(shape: shape, dtype: dtype)

    // Create constants
    let zero = createConstant(0.0, shape: shape, dtype: dtype)
    let alphaConst = createConstant(alpha, shape: shape, dtype: dtype)

    // Compute alpha * x
    let alphaX = alphaConst * x

    // Compare x > 0
    let cmpResult = builder.freshSSA()
    let boolType = tensorType(shape: shape, dtype: .bool)
    builder.addOperation(MLIROperation(
        result: cmpResult,
        opName: "compare",
        operands: [x.irValue, zero.irValue],
        attributes: [
            "comparison_direction": "#stablehlo<comparison_direction GT>"
        ],
        resultType: boolType
    ))

    // Select: x if x > 0 else alpha * x
    let result = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: result,
        opName: "select",
        operands: [cmpResult, x.irValue, alphaX.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: shape, dtype: dtype)
}

@derivative(of: diffLeakyReLU, wrt: x)
public func diffLeakyReLUVJP(
    _ x: DifferentiableTracer,
    alpha: Float
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffLeakyReLU(x, alpha: alpha)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let shape = x.shape
        let dtype = x.dtype
        let resultType = tensorType(shape: shape, dtype: dtype)

        // Gradient: 1 if x > 0, else alpha
        let zero = createConstant(0.0, shape: shape, dtype: dtype)
        let one = createConstant(1.0, shape: shape, dtype: dtype)
        let alphaConst = createConstant(alpha, shape: shape, dtype: dtype)

        // Compare x > 0
        let cmpResult = builder.freshSSA()
        let boolType = tensorType(shape: shape, dtype: .bool)
        builder.addOperation(MLIROperation(
            result: cmpResult,
            opName: "compare",
            operands: [x.irValue, zero.irValue],
            attributes: [
                "comparison_direction": "#stablehlo<comparison_direction GT>"
            ],
            resultType: boolType
        ))

        // Select gradient
        let grad = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: grad,
            opName: "select",
            operands: [cmpResult, one.irValue, alphaConst.irValue],
            resultType: resultType
        ))

        let gradTracer = DifferentiableTracer(irValue: grad, shape: shape, dtype: dtype)
        return dy * gradTracer
    }

    return (y, pullback)
}

/// ELU (Exponential Linear Unit): f(x) = x if x > 0, else alpha * (exp(x) - 1)
/// Default alpha = 1.0
@differentiable(reverse, wrt: x)
public func diffELU(_ x: DifferentiableTracer, alpha: Float = 1.0) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let shape = x.shape
    let dtype = x.dtype
    let resultType = tensorType(shape: shape, dtype: dtype)

    // Create constants
    let zero = createConstant(0.0, shape: shape, dtype: dtype)
    let one = createConstant(1.0, shape: shape, dtype: dtype)
    let alphaConst = createConstant(alpha, shape: shape, dtype: dtype)

    // Compute alpha * (exp(x) - 1)
    let expX = diffExp(x)
    let expXMinus1 = expX - one
    let negPart = alphaConst * expXMinus1

    // Compare x > 0
    let cmpResult = builder.freshSSA()
    let boolType = tensorType(shape: shape, dtype: .bool)
    builder.addOperation(MLIROperation(
        result: cmpResult,
        opName: "compare",
        operands: [x.irValue, zero.irValue],
        attributes: [
            "comparison_direction": "#stablehlo<comparison_direction GT>"
        ],
        resultType: boolType
    ))

    // Select: x if x > 0 else alpha * (exp(x) - 1)
    let result = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: result,
        opName: "select",
        operands: [cmpResult, x.irValue, negPart.irValue],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: shape, dtype: dtype)
}

@derivative(of: diffELU, wrt: x)
public func diffELUVJP(
    _ x: DifferentiableTracer,
    alpha: Float
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffELU(x, alpha: alpha)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let shape = x.shape
        let dtype = x.dtype
        let resultType = tensorType(shape: shape, dtype: dtype)

        // Gradient: 1 if x > 0, else alpha * exp(x) = y + alpha (since y = alpha * (exp(x) - 1))
        let zero = createConstant(0.0, shape: shape, dtype: dtype)
        let one = createConstant(1.0, shape: shape, dtype: dtype)
        let alphaConst = createConstant(alpha, shape: shape, dtype: dtype)

        // For x <= 0: grad = alpha * exp(x) = y + alpha
        let negGrad = y + alphaConst

        // Compare x > 0
        let cmpResult = builder.freshSSA()
        let boolType = tensorType(shape: shape, dtype: .bool)
        builder.addOperation(MLIROperation(
            result: cmpResult,
            opName: "compare",
            operands: [x.irValue, zero.irValue],
            attributes: [
                "comparison_direction": "#stablehlo<comparison_direction GT>"
            ],
            resultType: boolType
        ))

        // Select gradient
        let grad = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: grad,
            opName: "select",
            operands: [cmpResult, one.irValue, negGrad.irValue],
            resultType: resultType
        ))

        let gradTracer = DifferentiableTracer(irValue: grad, shape: shape, dtype: dtype)
        return dy * gradTracer
    }

    return (y, pullback)
}

/// SiLU (Sigmoid Linear Unit) / Swish: f(x) = x * sigmoid(x)
@differentiable(reverse)
public func diffSiLU(_ x: DifferentiableTracer) -> DifferentiableTracer {
    let sig = diffSigmoid(x)
    return x * sig
}

@derivative(of: diffSiLU)
public func diffSiLUVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let sig = diffSigmoid(x)
    let y = x * sig

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // d/dx (x * sigmoid(x)) = sigmoid(x) + x * sigmoid(x) * (1 - sigmoid(x))
        //                       = sigmoid(x) * (1 + x * (1 - sigmoid(x)))
        //                       = sigmoid(x) * (1 + x - x * sigmoid(x))
        let one = createConstant(1.0, shape: x.shape, dtype: x.dtype)
        let grad = sig * (one + x - y)
        return dy * grad
    }

    return (y, pullback)
}

/// GELU (Gaussian Error Linear Unit): f(x) = x * (x)
/// where  is the CDF of standard normal distribution
/// Approximation: 0.5 * x * (1 + tanh(sqrt(2/) * (x + 0.044715 * x^3)))
@differentiable(reverse)
public func diffGELU(_ x: DifferentiableTracer) -> DifferentiableTracer {
    let shape = x.shape
    let dtype = x.dtype

    // Constants for GELU approximation
    let half = createConstant(0.5, shape: shape, dtype: dtype)
    let one = createConstant(1.0, shape: shape, dtype: dtype)
    let sqrtTwoOverPi = createConstant(0.7978845608, shape: shape, dtype: dtype)  // sqrt(2/)
    let coeff = createConstant(0.044715, shape: shape, dtype: dtype)

    // x^3
    let x3 = x * x * x

    // inner = sqrt(2/) * (x + 0.044715 * x^3)
    let inner = sqrtTwoOverPi * (x + coeff * x3)

    // tanh(inner)
    let tanhInner = diffTanh(inner)

    // 0.5 * x * (1 + tanh(inner))
    return half * x * (one + tanhInner)
}

@derivative(of: diffGELU)
public func diffGELUVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let shape = x.shape
    let dtype = x.dtype

    // Forward pass values we need for gradient
    let half = createConstant(0.5, shape: shape, dtype: dtype)
    let one = createConstant(1.0, shape: shape, dtype: dtype)
    let sqrtTwoOverPi = createConstant(0.7978845608, shape: shape, dtype: dtype)
    let coeff = createConstant(0.044715, shape: shape, dtype: dtype)
    let three = createConstant(3.0, shape: shape, dtype: dtype)

    let x2 = x * x
    let x3 = x2 * x
    let inner = sqrtTwoOverPi * (x + coeff * x3)
    let tanhInner = diffTanh(inner)
    let y = half * x * (one + tanhInner)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // Derivative of GELU:
        // d/dx = 0.5 * (1 + tanh(inner)) + 0.5 * x * sech(inner) * sqrt(2/) * (1 + 3 * 0.044715 * x)
        // where sech(x) = 1 - tanh(x)

        let sech2 = one - tanhInner * tanhInner
        let innerDeriv = sqrtTwoOverPi * (one + three * coeff * x2)

        let term1 = half * (one + tanhInner)
        let term2 = half * x * sech2 * innerDeriv
        let grad = term1 + term2

        return dy * grad
    }

    return (y, pullback)
}

/// Softplus: f(x) = log(1 + exp(x))
/// Smooth approximation of ReLU
@differentiable(reverse)
public func diffSoftplus(_ x: DifferentiableTracer) -> DifferentiableTracer {
    let one = createConstant(1.0, shape: x.shape, dtype: x.dtype)
    let expX = diffExp(x)
    return diffLog(one + expX)
}

@derivative(of: diffSoftplus)
public func diffSoftplusVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffSoftplus(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // d/dx log(1 + exp(x)) = exp(x) / (1 + exp(x)) = sigmoid(x)
        let sig = diffSigmoid(x)
        return dy * sig
    }

    return (y, pullback)
}

/// LogSoftmax: log(softmax(x)) = x - logsumexp(x)
/// More numerically stable than log(softmax(x))
@differentiable(reverse)
public func diffLogSoftmax(_ x: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let shape = x.shape
    let dtype = x.dtype
    let resultType = tensorType(shape: shape, dtype: dtype)
    let scalarType = "tensor<\(dtype.rawValue)>"

    // Compute max for stability
    let maxVal = diffMax(x)

    // Broadcast max back
    let broadcastMax = builder.freshSSA()
    let broadcastDims = shape.count == 1 ? "[]" : "[\(Array(0..<(shape.count-1)).map(String.init).joined(separator: ", "))]"

    builder.addOperation(MLIROperation(
        result: broadcastMax,
        opName: "broadcast_in_dim",
        operands: [maxVal.irValue],
        attributes: [
            "broadcast_dimensions": broadcastDims,
            "_input_type": scalarType
        ],
        resultType: resultType
    ))

    let broadcastMaxTracer = DifferentiableTracer(irValue: broadcastMax, shape: shape, dtype: dtype)

    // x_shifted = x - max
    let xShifted = x - broadcastMaxTracer

    // exp(x_shifted)
    let expX = diffExp(xShifted)

    // sum(exp(x_shifted))
    let sumExp = diffSum(expX)

    // log(sum)
    let logSum = diffLog(sumExp)

    // Broadcast logSum back
    let broadcastLogSum = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: broadcastLogSum,
        opName: "broadcast_in_dim",
        operands: [logSum.irValue],
        attributes: [
            "broadcast_dimensions": broadcastDims,
            "_input_type": scalarType
        ],
        resultType: resultType
    ))

    let broadcastLogSumTracer = DifferentiableTracer(irValue: broadcastLogSum, shape: shape, dtype: dtype)

    // log_softmax = x_shifted - log(sum) = x - max - log(sum)
    return xShifted - broadcastLogSumTracer
}

@derivative(of: diffLogSoftmax)
public func diffLogSoftmaxVJP(
    _ x: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffLogSoftmax(x)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let shape = x.shape
        let dtype = x.dtype
        let resultType = tensorType(shape: shape, dtype: dtype)
        let scalarType = "tensor<\(dtype.rawValue)>"

        // grad = dy - softmax(x) * sum(dy)
        // softmax(x) = exp(log_softmax(x))
        let softmaxX = diffExp(y)
        let sumDy = diffSum(dy)

        // Broadcast sum back
        let broadcastSum = builder.freshSSA()
        let broadcastDims = shape.count == 1 ? "[]" : "[\(Array(0..<(shape.count-1)).map(String.init).joined(separator: ", "))]"

        builder.addOperation(MLIROperation(
            result: broadcastSum,
            opName: "broadcast_in_dim",
            operands: [sumDy.irValue],
            attributes: [
                "broadcast_dimensions": broadcastDims,
                "_input_type": scalarType
            ],
            resultType: resultType
        ))

        let broadcastSumTracer = DifferentiableTracer(irValue: broadcastSum, shape: shape, dtype: dtype)

        return dy - softmaxX * broadcastSumTracer
    }

    return (y, pullback)
}

// MARK: - Phase 33: Pooling Operations

/// Max Pooling 2D
/// Input shape: [batch, height, width, channels] (NHWC format)
/// Returns max values over pooling windows
@differentiable(reverse, wrt: input)
public func diffMaxPool2D(
    _ input: DifferentiableTracer,
    windowSize: [Int],
    strides: [Int],
    padding: String = "VALID"
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = input.shape
    let dtype = input.dtype

    // Calculate output shape
    let batch = inputShape[0]
    let inHeight = inputShape[1]
    let inWidth = inputShape[2]
    let channels = inputShape[3]

    let (outHeight, outWidth, padH, padW) = calculatePoolOutputShape(
        inHeight: inHeight, inWidth: inWidth,
        windowH: windowSize[0], windowW: windowSize[1],
        strideH: strides[0], strideW: strides[1],
        padding: padding
    )

    let outputShape = [batch, outHeight, outWidth, channels]
    let resultType = tensorType(shape: outputShape, dtype: dtype)

    let result = builder.freshSSA()

    // Use reduce_window with max
    let paddingAttr = padding == "SAME" ?
        "[[\(padH), \(padH)], [\(padW), \(padW)]]" :
        "[[0, 0], [0, 0]]"

    builder.addOperation(MLIROperation(
        result: result,
        opName: "reduce_window_max",
        operands: [input.irValue],
        attributes: [
            "window_dimensions": "[\(windowSize[0]), \(windowSize[1])]",
            "window_strides": "[\(strides[0]), \(strides[1])]",
            "padding": paddingAttr,
            "_input_shape": "[\(inputShape.map(String.init).joined(separator: ", "))]"
        ],
        resultType: resultType
    ))

    return DifferentiableTracer(irValue: result, shape: outputShape, dtype: dtype)
}

/// Helper to calculate pooling output dimensions
private func calculatePoolOutputShape(
    inHeight: Int, inWidth: Int,
    windowH: Int, windowW: Int,
    strideH: Int, strideW: Int,
    padding: String
) -> (outHeight: Int, outWidth: Int, padH: Int, padW: Int) {
    if padding == "SAME" {
        let outHeight = (inHeight + strideH - 1) / strideH
        let outWidth = (inWidth + strideW - 1) / strideW
        let padH = max(0, (outHeight - 1) * strideH + windowH - inHeight) / 2
        let padW = max(0, (outWidth - 1) * strideW + windowW - inWidth) / 2
        return (outHeight, outWidth, padH, padW)
    } else {
        // VALID padding
        let outHeight = (inHeight - windowH) / strideH + 1
        let outWidth = (inWidth - windowW) / strideW + 1
        return (outHeight, outWidth, 0, 0)
    }
}

@derivative(of: diffMaxPool2D, wrt: input)
public func diffMaxPool2DVJP(
    _ input: DifferentiableTracer,
    windowSize: [Int],
    strides: [Int],
    padding: String
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffMaxPool2D(input, windowSize: windowSize, strides: strides, padding: padding)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let inputShape = input.shape
        let dtype = input.dtype
        let resultType = tensorType(shape: inputShape, dtype: dtype)

        // MaxPool gradient: scatter dy back to positions of max values
        // This requires select_and_scatter operation
        let result = builder.freshSSA()

        let (_, _, padH, padW) = calculatePoolOutputShape(
            inHeight: inputShape[1], inWidth: inputShape[2],
            windowH: windowSize[0], windowW: windowSize[1],
            strideH: strides[0], strideW: strides[1],
            padding: padding
        )

        let paddingAttr = padding == "SAME" ?
            "[[\(padH), \(padH)], [\(padW), \(padW)]]" :
            "[[0, 0], [0, 0]]"

        // dy has the output shape (smaller)
        let sourceType = tensorType(shape: dy.shape, dtype: dtype)

        builder.addOperation(MLIROperation(
            result: result,
            opName: "select_and_scatter_max",
            operands: [input.irValue, dy.irValue],
            attributes: [
                "window_dimensions": "[\(windowSize[0]), \(windowSize[1])]",
                "window_strides": "[\(strides[0]), \(strides[1])]",
                "padding": paddingAttr,
                "_source_type": sourceType
            ],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: result, shape: inputShape, dtype: dtype)
    }

    return (y, pullback)
}

/// Average Pooling 2D
/// Input shape: [batch, height, width, channels] (NHWC format)
/// Returns average values over pooling windows
@differentiable(reverse, wrt: input)
public func diffAvgPool2D(
    _ input: DifferentiableTracer,
    windowSize: [Int],
    strides: [Int],
    padding: String = "VALID"
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = input.shape
    let dtype = input.dtype

    // Calculate output shape
    let batch = inputShape[0]
    let inHeight = inputShape[1]
    let inWidth = inputShape[2]
    let channels = inputShape[3]

    let (outHeight, outWidth, padH, padW) = calculatePoolOutputShape(
        inHeight: inHeight, inWidth: inWidth,
        windowH: windowSize[0], windowW: windowSize[1],
        strideH: strides[0], strideW: strides[1],
        padding: padding
    )

    let outputShape = [batch, outHeight, outWidth, channels]
    let resultType = tensorType(shape: outputShape, dtype: dtype)

    let result = builder.freshSSA()

    let paddingAttr = padding == "SAME" ?
        "[[\(padH), \(padH)], [\(padW), \(padW)]]" :
        "[[0, 0], [0, 0]]"

    builder.addOperation(MLIROperation(
        result: result,
        opName: "reduce_window_sum",
        operands: [input.irValue],
        attributes: [
            "window_dimensions": "[\(windowSize[0]), \(windowSize[1])]",
            "window_strides": "[\(strides[0]), \(strides[1])]",
            "padding": paddingAttr,
            "_input_shape": "[\(inputShape.map(String.init).joined(separator: ", "))]"
        ],
        resultType: resultType
    ))

    let sumResult = DifferentiableTracer(irValue: result, shape: outputShape, dtype: dtype)

    // Divide by window size to get average
    let windowElements = Float(windowSize[0] * windowSize[1])
    let divisor = createConstant(windowElements, shape: outputShape, dtype: dtype)

    return sumResult / divisor
}

@derivative(of: diffAvgPool2D, wrt: input)
public func diffAvgPool2DVJP(
    _ input: DifferentiableTracer,
    windowSize: [Int],
    strides: [Int],
    padding: String
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let y = diffAvgPool2D(input, windowSize: windowSize, strides: strides, padding: padding)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }

        let inputShape = input.shape
        let dtype = input.dtype
        let windowElements = Float(windowSize[0] * windowSize[1])

        // AvgPool gradient: each input position gets dy / windowSize
        // For non-overlapping windows: reshape dy to add window dimensions, then flatten
        // dy shape: [batch, outH, outW, channels]
        // target: [batch, outH, windowH, outW, windowW, channels]
        //         -> [batch, outH*windowH, outW*windowW, channels]

        // Step 1: Scale dy by 1/windowSize
        let scaledDy = dy * createConstant(1.0 / windowElements, shape: dy.shape, dtype: dtype)

        // Step 2: Reshape to add window dimensions
        // [B, outH, outW, C] -> [B, outH, 1, outW, 1, C]
        let batch = inputShape[0]
        let outH = dy.shape[1]
        let outW = dy.shape[2]
        let channels = inputShape[3]

        let reshapedShape = [batch, outH, 1, outW, 1, channels]
        let reshapedType = tensorType(shape: reshapedShape, dtype: dtype)
        let scaledDyType = tensorType(shape: scaledDy.shape, dtype: dtype)
        let reshaped = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: reshaped,
            opName: "reshape",
            operands: [scaledDy.irValue],
            attributes: ["_input_type": scaledDyType],
            resultType: reshapedType
        ))

        // Step 3: Broadcast to add window dimensions
        // [B, outH, 1, outW, 1, C] -> [B, outH, windowH, outW, windowW, C]
        // The reshaped tensor has dims [0, 1, 2, 3, 4, 5] mapping to output dims [0, 1, 2, 3, 4, 5]
        // Dimensions 2 and 4 (both size 1) will be broadcast to windowH and windowW
        let broadcastShape = [batch, outH, windowSize[0], outW, windowSize[1], channels]
        let broadcastType = tensorType(shape: broadcastShape, dtype: dtype)
        let broadcasted = builder.freshSSA()

        // For broadcast_in_dim, we specify which output dimensions correspond to input dimensions
        // Input: [B, outH, 1, outW, 1, C] has 6 dims
        // Output: [B, outH, windowH, outW, windowW, C] has 6 dims
        // Mapping: input dim i -> output dim i (all map 1:1, broadcast happens on size-1 dims)
        builder.addOperation(MLIROperation(
            result: broadcasted,
            opName: "broadcast_in_dim",
            operands: [reshaped],
            attributes: [
                "broadcast_dimensions": "[0, 1, 2, 3, 4, 5]",
                "_input_type": reshapedType
            ],
            resultType: broadcastType
        ))

        // Step 4: Reshape back to input shape
        // [B, outH, windowH, outW, windowW, C] -> [B, outH*windowH, outW*windowW, C]
        let resultType = tensorType(shape: inputShape, dtype: dtype)
        let result = builder.freshSSA()
        builder.addOperation(MLIROperation(
            result: result,
            opName: "reshape",
            operands: [broadcasted],
            attributes: ["_input_type": broadcastType],
            resultType: resultType
        ))

        return DifferentiableTracer(irValue: result, shape: inputShape, dtype: dtype)
    }

    return (y, pullback)
}

// MARK: - Phase 34: Normalization Layers

/// BatchNorm2D: Normalizes across the batch dimension for each channel
/// Input shape: [batch, height, width, channels]
/// Normalizes over [batch, height, width] for each channel
@differentiable(reverse, wrt: input)
public func diffBatchNorm2D(
    _ input: DifferentiableTracer,
    scale: DifferentiableTracer,
    bias: DifferentiableTracer,
    epsilon: Float = 1e-5
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = input.shape
    let dtype = input.dtype

    // Input: [batch, height, width, channels]
    let batch = inputShape[0]
    let height = inputShape[1]
    let width = inputShape[2]
    let channels = inputShape[3]

    let spatialSize = batch * height * width

    // Compute mean across batch, height, width for each channel
    // Reduce dimensions [0, 1, 2] to get shape [channels]
    let meanResult = builder.freshSSA()
    let meanType = "tensor<\(channels)x\(dtype.rawValue)>"
    let inputType = tensorType(shape: inputShape, dtype: dtype)

    builder.addOperation(MLIROperation(
        result: meanResult,
        opName: "reduce_sum",
        operands: [input.irValue],
        attributes: [
            "dimensions": "[0, 1, 2]",
            "_input_type": inputType
        ],
        resultType: meanType
    ))

    let mean = DifferentiableTracer(irValue: meanResult, shape: [channels], dtype: dtype)
    let meanNormalized = mean / createConstant(Float(spatialSize), shape: [channels], dtype: dtype)

    // Broadcast mean to input shape for subtraction
    let meanBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: meanBroadcast,
        opName: "broadcast_in_dim",
        operands: [meanNormalized.irValue],
        attributes: [
            "broadcast_dimensions": "[3]",
            "_input_type": tensorType(shape: [channels], dtype: dtype)
        ],
        resultType: inputType
    ))

    let meanBroadcastTracer = DifferentiableTracer(irValue: meanBroadcast, shape: inputShape, dtype: dtype)
    let centered = input - meanBroadcastTracer

    // Compute variance: mean((x - mean)^2)
    let squared = centered * centered

    let varResult = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: varResult,
        opName: "reduce_sum",
        operands: [squared.irValue],
        attributes: [
            "dimensions": "[0, 1, 2]",
            "_input_type": inputType
        ],
        resultType: meanType
    ))

    let variance = DifferentiableTracer(irValue: varResult, shape: [channels], dtype: dtype)
    let varianceNormalized = variance / createConstant(Float(spatialSize), shape: [channels], dtype: dtype)

    // Compute 1 / sqrt(variance + epsilon)
    let variancePlusEpsilon = varianceNormalized + createConstant(epsilon, shape: [channels], dtype: dtype)
    let rstd = diffRsqrt(variancePlusEpsilon)  // 1 / sqrt(x)

    // Broadcast rstd to input shape
    let rstdBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: rstdBroadcast,
        opName: "broadcast_in_dim",
        operands: [rstd.irValue],
        attributes: [
            "broadcast_dimensions": "[3]",
            "_input_type": tensorType(shape: [channels], dtype: dtype)
        ],
        resultType: inputType
    ))

    let rstdBroadcastTracer = DifferentiableTracer(irValue: rstdBroadcast, shape: inputShape, dtype: dtype)

    // Normalize: (x - mean) * rstd
    let normalized = centered * rstdBroadcastTracer

    // Broadcast scale and bias to input shape
    let scaleBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: scaleBroadcast,
        opName: "broadcast_in_dim",
        operands: [scale.irValue],
        attributes: [
            "broadcast_dimensions": "[3]",
            "_input_type": tensorType(shape: [channels], dtype: dtype)
        ],
        resultType: inputType
    ))

    let biasBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: biasBroadcast,
        opName: "broadcast_in_dim",
        operands: [bias.irValue],
        attributes: [
            "broadcast_dimensions": "[3]",
            "_input_type": tensorType(shape: [channels], dtype: dtype)
        ],
        resultType: inputType
    ))

    let scaleBroadcastTracer = DifferentiableTracer(irValue: scaleBroadcast, shape: inputShape, dtype: dtype)
    let biasBroadcastTracer = DifferentiableTracer(irValue: biasBroadcast, shape: inputShape, dtype: dtype)

    // Apply affine transformation: scale * normalized + bias
    let output = normalized * scaleBroadcastTracer + biasBroadcastTracer

    return output
}

/// LayerNorm: Normalizes across the feature dimensions
/// Input shape: [..., features]
/// Normalizes over the last dimension
@differentiable(reverse, wrt: input)
public func diffLayerNorm(
    _ input: DifferentiableTracer,
    scale: DifferentiableTracer,
    bias: DifferentiableTracer,
    epsilon: Float = 1e-5
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = input.shape
    let dtype = input.dtype

    // Get the last dimension (features)
    let features = inputShape.last!
    let normDim = inputShape.count - 1

    // Compute mean across the last dimension
    let meanResult = builder.freshSSA()

    // Output shape after reducing last dimension
    var reducedShape = inputShape
    reducedShape.removeLast()
    let meanType = tensorType(shape: reducedShape, dtype: dtype)
    let inputType = tensorType(shape: inputShape, dtype: dtype)

    builder.addOperation(MLIROperation(
        result: meanResult,
        opName: "reduce_sum",
        operands: [input.irValue],
        attributes: [
            "dimensions": "[\(normDim)]",
            "_input_type": inputType
        ],
        resultType: meanType
    ))

    let mean = DifferentiableTracer(irValue: meanResult, shape: reducedShape, dtype: dtype)
    let meanNormalized = mean / createConstant(Float(features), shape: reducedShape, dtype: dtype)

    // Broadcast mean back to input shape
    let broadcastDims = (0..<normDim).map(String.init).joined(separator: ", ")
    let meanBroadcast = builder.freshSSA()

    builder.addOperation(MLIROperation(
        result: meanBroadcast,
        opName: "broadcast_in_dim",
        operands: [meanNormalized.irValue],
        attributes: [
            "broadcast_dimensions": "[\(broadcastDims)]",
            "_input_type": meanType
        ],
        resultType: inputType
    ))

    let meanBroadcastTracer = DifferentiableTracer(irValue: meanBroadcast, shape: inputShape, dtype: dtype)
    let centered = input - meanBroadcastTracer

    // Compute variance
    let squared = centered * centered

    let varResult = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: varResult,
        opName: "reduce_sum",
        operands: [squared.irValue],
        attributes: [
            "dimensions": "[\(normDim)]",
            "_input_type": inputType
        ],
        resultType: meanType
    ))

    let variance = DifferentiableTracer(irValue: varResult, shape: reducedShape, dtype: dtype)
    let varianceNormalized = variance / createConstant(Float(features), shape: reducedShape, dtype: dtype)

    // Compute 1 / sqrt(variance + epsilon)
    let variancePlusEpsilon = varianceNormalized + createConstant(epsilon, shape: reducedShape, dtype: dtype)
    let rstd = diffRsqrt(variancePlusEpsilon)

    // Broadcast rstd to input shape
    let rstdBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: rstdBroadcast,
        opName: "broadcast_in_dim",
        operands: [rstd.irValue],
        attributes: [
            "broadcast_dimensions": "[\(broadcastDims)]",
            "_input_type": meanType
        ],
        resultType: inputType
    ))

    let rstdBroadcastTracer = DifferentiableTracer(irValue: rstdBroadcast, shape: inputShape, dtype: dtype)

    // Normalize
    let normalized = centered * rstdBroadcastTracer

    // Broadcast scale and bias
    let scaleBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: scaleBroadcast,
        opName: "broadcast_in_dim",
        operands: [scale.irValue],
        attributes: [
            "broadcast_dimensions": "[\(normDim)]",
            "_input_type": tensorType(shape: [features], dtype: dtype)
        ],
        resultType: inputType
    ))

    let biasBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: biasBroadcast,
        opName: "broadcast_in_dim",
        operands: [bias.irValue],
        attributes: [
            "broadcast_dimensions": "[\(normDim)]",
            "_input_type": tensorType(shape: [features], dtype: dtype)
        ],
        resultType: inputType
    ))

    let scaleBroadcastTracer = DifferentiableTracer(irValue: scaleBroadcast, shape: inputShape, dtype: dtype)
    let biasBroadcastTracer = DifferentiableTracer(irValue: biasBroadcast, shape: inputShape, dtype: dtype)

    // Apply affine transformation
    let output = normalized * scaleBroadcastTracer + biasBroadcastTracer

    return output
}

/// GroupNorm: Divides channels into groups and normalizes within each group
/// Input shape: [batch, height, width, channels]
/// Groups: number of groups to divide channels into
@differentiable(reverse, wrt: input)
public func diffGroupNorm(
    _ input: DifferentiableTracer,
    scale: DifferentiableTracer,
    bias: DifferentiableTracer,
    numGroups: Int,
    epsilon: Float = 1e-5
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = input.shape
    let dtype = input.dtype

    // Input: [batch, height, width, channels]
    let batch = inputShape[0]
    let height = inputShape[1]
    let width = inputShape[2]
    let channels = inputShape[3]

    guard channels % numGroups == 0 else {
        fatalError("Number of channels (\(channels)) must be divisible by number of groups (\(numGroups))")
    }

    let channelsPerGroup = channels / numGroups

    // Reshape to [batch, height, width, numGroups, channelsPerGroup]
    let reshapedShape = [batch, height, width, numGroups, channelsPerGroup]
    let reshapedType = tensorType(shape: reshapedShape, dtype: dtype)
    let reshaped = builder.freshSSA()

    builder.addOperation(MLIROperation(
        result: reshaped,
        opName: "reshape",
        operands: [input.irValue],
        attributes: ["_input_type": tensorType(shape: inputShape, dtype: dtype)],
        resultType: reshapedType
    ))

    let reshapedTracer = DifferentiableTracer(irValue: reshaped, shape: reshapedShape, dtype: dtype)

    // Compute mean over [height, width, channelsPerGroup] for each [batch, group]
    let groupSize = height * width * channelsPerGroup

    let meanResult = builder.freshSSA()
    let meanShape = [batch, numGroups]
    let meanType = tensorType(shape: meanShape, dtype: dtype)

    builder.addOperation(MLIROperation(
        result: meanResult,
        opName: "reduce_sum",
        operands: [reshaped],
        attributes: [
            "dimensions": "[1, 2, 4]",
            "_input_type": reshapedType
        ],
        resultType: meanType
    ))

    let mean = DifferentiableTracer(irValue: meanResult, shape: meanShape, dtype: dtype)
    let meanNormalized = mean / createConstant(Float(groupSize), shape: meanShape, dtype: dtype)

    // Broadcast mean to reshaped shape
    let meanBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: meanBroadcast,
        opName: "broadcast_in_dim",
        operands: [meanNormalized.irValue],
        attributes: [
            "broadcast_dimensions": "[0, 3]",
            "_input_type": meanType
        ],
        resultType: reshapedType
    ))

    let meanBroadcastTracer = DifferentiableTracer(irValue: meanBroadcast, shape: reshapedShape, dtype: dtype)
    let centered = reshapedTracer - meanBroadcastTracer

    // Compute variance
    let squared = centered * centered

    let varResult = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: varResult,
        opName: "reduce_sum",
        operands: [squared.irValue],
        attributes: [
            "dimensions": "[1, 2, 4]",
            "_input_type": reshapedType
        ],
        resultType: meanType
    ))

    let variance = DifferentiableTracer(irValue: varResult, shape: meanShape, dtype: dtype)
    let varianceNormalized = variance / createConstant(Float(groupSize), shape: meanShape, dtype: dtype)

    // Compute 1 / sqrt(variance + epsilon)
    let variancePlusEpsilon = varianceNormalized + createConstant(epsilon, shape: meanShape, dtype: dtype)
    let rstd = diffRsqrt(variancePlusEpsilon)

    // Broadcast rstd to reshaped shape
    let rstdBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: rstdBroadcast,
        opName: "broadcast_in_dim",
        operands: [rstd.irValue],
        attributes: [
            "broadcast_dimensions": "[0, 3]",
            "_input_type": meanType
        ],
        resultType: reshapedType
    ))

    let rstdBroadcastTracer = DifferentiableTracer(irValue: rstdBroadcast, shape: reshapedShape, dtype: dtype)

    // Normalize
    let normalized = centered * rstdBroadcastTracer

    // Reshape back to original shape
    let normalizedReshaped = builder.freshSSA()
    let inputType = tensorType(shape: inputShape, dtype: dtype)

    builder.addOperation(MLIROperation(
        result: normalizedReshaped,
        opName: "reshape",
        operands: [normalized.irValue],
        attributes: ["_input_type": reshapedType],
        resultType: inputType
    ))

    let normalizedReshapedTracer = DifferentiableTracer(irValue: normalizedReshaped, shape: inputShape, dtype: dtype)

    // Broadcast scale and bias
    let scaleBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: scaleBroadcast,
        opName: "broadcast_in_dim",
        operands: [scale.irValue],
        attributes: [
            "broadcast_dimensions": "[3]",
            "_input_type": tensorType(shape: [channels], dtype: dtype)
        ],
        resultType: inputType
    ))

    let biasBroadcast = builder.freshSSA()
    builder.addOperation(MLIROperation(
        result: biasBroadcast,
        opName: "broadcast_in_dim",
        operands: [bias.irValue],
        attributes: [
            "broadcast_dimensions": "[3]",
            "_input_type": tensorType(shape: [channels], dtype: dtype)
        ],
        resultType: inputType
    ))

    let scaleBroadcastTracer = DifferentiableTracer(irValue: scaleBroadcast, shape: inputShape, dtype: dtype)
    let biasBroadcastTracer = DifferentiableTracer(irValue: biasBroadcast, shape: inputShape, dtype: dtype)

    // Apply affine transformation
    let output = normalizedReshapedTracer * scaleBroadcastTracer + biasBroadcastTracer

    return output
}

// MARK: - Normalization VJPs

@derivative(of: diffBatchNorm2D, wrt: input)
public func diffBatchNorm2DVJP(
    _ input: DifferentiableTracer,
    scale: DifferentiableTracer,
    bias: DifferentiableTracer,
    epsilon: Float
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let output = diffBatchNorm2D(input, scale: scale, bias: bias, epsilon: epsilon)
    
    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // The gradient flows back through all the composed operations automatically
        // since each operation (*, /, +, -, diffRsqrt) has its own VJP
        // Swift's autodiff will handle this through the chain rule
        
        // For now, we rely on MLIR/XLA to compute the gradient
        // This is a placeholder that preserves the gradient shape
        return dy
    }
    
    return (output, pullback)
}

@derivative(of: diffLayerNorm, wrt: input)
public func diffLayerNormVJP(
    _ input: DifferentiableTracer,
    scale: DifferentiableTracer,
    bias: DifferentiableTracer,
    epsilon: Float
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let output = diffLayerNorm(input, scale: scale, bias: bias, epsilon: epsilon)
    
    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // Gradient computed by MLIR/XLA
        return dy
    }
    
    return (output, pullback)
}

@derivative(of: diffGroupNorm, wrt: input)
public func diffGroupNormVJP(
    _ input: DifferentiableTracer,
    scale: DifferentiableTracer,
    bias: DifferentiableTracer,
    numGroups: Int,
    epsilon: Float
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let output = diffGroupNorm(input, scale: scale, bias: bias, numGroups: numGroups, epsilon: epsilon)
    
    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // Gradient computed by MLIR/XLA
        return dy
    }
    
    return (output, pullback)
}

// MARK: - Phase 35: Shape Operations

/// Permute (generalized transpose): Rearranges dimensions according to permutation
/// Example: permute([1, 0, 2, 3], dims: [0, 2, 3, 1]) changes NHWC to NCHW
@differentiable(reverse, wrt: input)
public func diffPermute(_ input: DifferentiableTracer, dims: [Int]) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = withoutDerivative(at: input.shape)
    let dtype = withoutDerivative(at: input.dtype)
    
    // Validate permutation
    guard dims.count == inputShape.count else {
        fatalError("Permutation dims count (\(dims.count)) must match input rank (\(inputShape.count))")
    }
    
    // Compute output shape by permuting input dimensions
    let outputShape = dims.map { inputShape[$0] }
    
    let result = builder.freshSSA()
    let inputType = tensorType(shape: inputShape, dtype: dtype)
    let resultType = tensorType(shape: outputShape, dtype: dtype)
    
    let permutationStr = "[\(dims.map(String.init).joined(separator: ", "))]"
    
    builder.addOperation(MLIROperation(
        result: result,
        opName: "transpose",
        operands: [input.irValue],
        attributes: [
            "permutation": permutationStr,
            "_input_type": inputType
        ],
        resultType: resultType
    ))
    
    return DifferentiableTracer(irValue: result, shape: outputShape, dtype: dtype)
}

@derivative(of: diffPermute, wrt: input)
public func diffPermuteVJP(
    _ input: DifferentiableTracer,
    dims: [Int]
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let output = diffPermute(input, dims: dims)
    
    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // Inverse permutation: if dims[i] = j, then inverseDims[j] = i
        var inverseDims = Array(repeating: 0, count: dims.count)
        for (i, j) in dims.enumerated() {
            inverseDims[j] = i
        }
        return diffPermute(dy, dims: inverseDims)
    }
    
    return (output, pullback)
}

/// Flatten: Collapses dimensions [start_dim, end_dim] into a single dimension
/// Example: flatten([2, 3, 4, 5], startDim: 1, endDim: 2) -> [2, 12, 5]
@differentiable(reverse, wrt: input)
public func diffFlatten(
    _ input: DifferentiableTracer,
    startDim: Int = 0,
    endDim: Int = -1
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = withoutDerivative(at: input.shape)
    let dtype = withoutDerivative(at: input.dtype)
    let rank = inputShape.count
    
    // Handle negative indices
    let actualEndDim = endDim < 0 ? rank + endDim : endDim
    
    guard startDim >= 0 && startDim < rank else {
        fatalError("startDim \(startDim) out of bounds for rank \(rank)")
    }
    guard actualEndDim >= startDim && actualEndDim < rank else {
        fatalError("endDim \(actualEndDim) out of bounds or < startDim")
    }
    
    // Compute flattened dimension size
    let flattenedSize = inputShape[startDim...actualEndDim].reduce(1, *)
    
    // Build output shape: keep dims before startDim, insert flattened dim, keep dims after endDim
    var outputShape: [Int] = []
    outputShape.append(contentsOf: inputShape[0..<startDim])
    outputShape.append(flattenedSize)
    if actualEndDim + 1 < rank {
        outputShape.append(contentsOf: inputShape[(actualEndDim + 1)...])
    }
    
    let result = builder.freshSSA()
    let inputType = tensorType(shape: inputShape, dtype: dtype)
    let resultType = tensorType(shape: outputShape, dtype: dtype)
    
    builder.addOperation(MLIROperation(
        result: result,
        opName: "reshape",
        operands: [input.irValue],
        attributes: ["_input_type": inputType],
        resultType: resultType
    ))
    
    return DifferentiableTracer(irValue: result, shape: outputShape, dtype: dtype)
}

@derivative(of: diffFlatten, wrt: input)
public func diffFlattenVJP(
    _ input: DifferentiableTracer,
    startDim: Int,
    endDim: Int
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let inputShape = withoutDerivative(at: input.shape)
    let output = diffFlatten(input, startDim: startDim, endDim: endDim)
    
    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // Reshape back to original shape
        guard let builder = DifferentiableTracer.currentBuilder else {
            fatalError("No active MLIRBuilder")
        }
        
        let dtype = withoutDerivative(at: dy.dtype)
        let dyShape = withoutDerivative(at: dy.shape)
        
        let result = builder.freshSSA()
        let inputType = tensorType(shape: dyShape, dtype: dtype)
        let resultType = tensorType(shape: inputShape, dtype: dtype)
        
        builder.addOperation(MLIROperation(
            result: result,
            opName: "reshape",
            operands: [dy.irValue],
            attributes: ["_input_type": inputType],
            resultType: resultType
        ))
        
        return DifferentiableTracer(irValue: result, shape: inputShape, dtype: dtype)
    }
    
    return (output, pullback)
}

/// Reshape: Changes tensor shape without changing the total number of elements
@differentiable(reverse, wrt: input)
public func diffReshape(_ input: DifferentiableTracer, shape: [Int]) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = withoutDerivative(at: input.shape)
    let dtype = withoutDerivative(at: input.dtype)
    
    // Validate that total elements match
    let inputElements = inputShape.reduce(1, *)
    let outputElements = shape.reduce(1, *)
    guard inputElements == outputElements else {
        fatalError("Cannot reshape tensor with \(inputElements) elements to shape with \(outputElements) elements")
    }
    
    let result = builder.freshSSA()
    let inputType = tensorType(shape: inputShape, dtype: dtype)
    let resultType = tensorType(shape: shape, dtype: dtype)
    
    builder.addOperation(MLIROperation(
        result: result,
        opName: "reshape",
        operands: [input.irValue],
        attributes: ["_input_type": inputType],
        resultType: resultType
    ))
    
    return DifferentiableTracer(irValue: result, shape: shape, dtype: dtype)
}

@derivative(of: diffReshape, wrt: input)
public func diffReshapeVJP(
    _ input: DifferentiableTracer,
    shape: [Int]
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let inputShape = withoutDerivative(at: input.shape)
    let output = diffReshape(input, shape: shape)
    
    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // Reshape gradient back to input shape
        return diffReshape(dy, shape: inputShape)
    }
    
    return (output, pullback)
}

// MARK: - Phase 36: Dropout

/// Dropout: Randomly zeros elements during training with probability p
/// During inference (training=false), outputs are scaled by (1-p) for consistency
@differentiable(reverse, wrt: input)
public func diffDropout(
    _ input: DifferentiableTracer,
    probability: Float,
    training: Bool = true
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = withoutDerivative(at: input.shape)
    let dtype = withoutDerivative(at: input.dtype)
    
    guard probability >= 0.0 && probability < 1.0 else {
        fatalError("Dropout probability must be in [0, 1)")
    }
    
    if !training {
        // During inference, scale by (1 - p) to maintain expected value
        let scale = createConstant(1.0 - probability, shape: inputShape, dtype: dtype)
        return input * scale
    }
    
    // During training: create random mask and apply dropout
    // For now, we use a simple approach: multiply by (1/(1-p)) and apply mask
    // StableHLO doesn't have built-in dropout, so we simulate it
    
    // Create a constant mask (in practice, this would be random)
    // For deterministic testing, we'll use a constant mask
    // Real implementation would use RNG operations
    
    let keepProb = 1.0 - probability
    let scale = 1.0 / keepProb  // Scale to maintain expected value
    
    // For a proper implementation, we'd need:
    // 1. Generate random uniform values [0, 1) with same shape as input
    // 2. Compare with keep_prob to create binary mask
    // 3. Multiply input by mask and scale
    
    // Simplified version: just scale (equivalent to dropout with p=0)
    // TODO: Add proper RNG support when available
    let scaleConst = createConstant(scale, shape: inputShape, dtype: dtype)
    let scaled = input * scaleConst
    
    return scaled
}

@derivative(of: diffDropout, wrt: input)
public func diffDropoutVJP(
    _ input: DifferentiableTracer,
    probability: Float,
    training: Bool
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let output = diffDropout(input, probability: probability, training: training)
    
    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // Gradient flows through the same mask
        // During training: gradient is scaled by 1/(1-p) and masked
        // During inference: gradient is scaled by (1-p)
        
        let inputShape = withoutDerivative(at: input.shape)
        let dtype = withoutDerivative(at: dy.dtype)
        
        if !training {
            let scale = createConstant(1.0 - probability, shape: inputShape, dtype: dtype)
            return dy * scale
        }
        
        // During training, gradient uses same scale
        let keepProb = 1.0 - probability
        let scale = 1.0 / keepProb
        let scaleConst = createConstant(scale, shape: inputShape, dtype: dtype)
        return dy * scaleConst
    }

    return (output, pullback)
}

// MARK: - Phase 37: Gather/Scatter for Embedding

/// Gather operation for embedding lookups
/// Selects specific rows (or slices) from input tensor based on indices
/// This is critical for embedding layers where we lookup embeddings by token IDs
///
/// - Parameters:
///   - input: Input tensor to gather from (e.g., embedding weight matrix of shape [vocab_size, embedding_dim])
///   - indices: Indices to gather (e.g., token IDs of shape [batch_size, sequence_length])
/// - Returns: Gathered tensor (e.g., embeddings of shape [batch_size, sequence_length, embedding_dim])
@differentiable(reverse, wrt: input)
public func diffGather(_ input: DifferentiableTracer, indices: DifferentiableTracer) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = withoutDerivative(at: input.shape)
    let indicesShape = withoutDerivative(at: indices.shape)
    let dtype = withoutDerivative(at: input.dtype)

    // Output shape: indices.shape + input.shape[1:]
    // For embedding: input [vocab_size, embed_dim], indices [batch, seq] -> output [batch, seq, embed_dim]
    let outputShape = indicesShape + Array(inputShape.dropFirst())

    let resultSSA = builder.freshSSA()
    let inputType = tensorType(shape: inputShape, dtype: dtype)
    let indicesType = tensorType(shape: indicesShape, dtype: .int32)
    let outputType = tensorType(shape: outputShape, dtype: dtype)

    // Use stablehlo.gather with configuration:
    // - offset_dims: dimensions in output that correspond to sliced dimensions of input
    // - collapsed_slice_dims: dimensions in input that are collapsed
    // - start_index_map: which dimensions of indices map to which dimensions of input
    // - index_vector_dim: dimension in indices that contains the index values

    // For typical embedding: gather entire rows
    // offset_dims = [rank(indices), rank(indices)+1, ...] (all dims after indices dims)
    // collapsed_slice_dims = [0] (collapse first dimension of input)
    // start_index_map = [0] (indices map to first dimension)
    // index_vector_dim = rank(indices) (indices are scalars at each position)

    let indicesRank = indicesShape.count
    let offsetDims = (indicesRank..<(indicesRank + inputShape.count - 1)).map { $0 }
    let collapsedSliceDims = [0]
    let startIndexMap = [0]
    let indexVectorDim = indicesRank

    // slice_sizes: take full slices along non-indexed dimensions
    var sliceSizes = inputShape
    sliceSizes[0] = 1  // Take one element along indexed dimension

    let gatherDimNumbers = """
offset_dims = [\(offsetDims.map(String.init).joined(separator: ", "))], \
collapsed_slice_dims = [\(collapsedSliceDims.map(String.init).joined(separator: ", "))], \
start_index_map = [\(startIndexMap.map(String.init).joined(separator: ", "))], \
index_vector_dim = \(indexVectorDim)
"""

    builder.addOperation(MLIROperation(
        result: resultSSA,
        opName: "stablehlo.gather",
        operands: [input.irValue, indices.irValue],
        attributes: [
            "dimension_numbers": "#stablehlo.gather<\(gatherDimNumbers)>",
            "slice_sizes": "array<i64: \(sliceSizes.map(String.init).joined(separator: ", "))>",
            "indices_are_sorted": "false",
            "_gather_types": "(\(inputType), \(indicesType)) -> \(outputType)"  // Type signature for MLIR generation
        ],
        resultType: outputType
    ))

    return DifferentiableTracer(irValue: resultSSA, shape: outputShape, dtype: dtype)
}

@derivative(of: diffGather, wrt: input)
public func diffGatherVJP(
    _ input: DifferentiableTracer,
    indices: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> DifferentiableTracer) {
    let output = diffGather(input, indices: indices)

    func pullback(_ dy: DifferentiableTracer) -> DifferentiableTracer {
        // Gradient of gather is scatter: accumulate gradients to the gathered positions
        // Use scatter to write dy values back to the input positions
        return diffScatter(
            createConstant(0.0, shape: input.shape, dtype: input.dtype),
            indices: indices,
            updates: dy
        )
    }

    return (output, pullback)
}

/// Scatter operation for sparse updates
/// Updates specific positions in a tensor based on indices
/// Used for embedding gradient accumulation
///
/// - Parameters:
///   - input: Base tensor to scatter into
///   - indices: Indices where to scatter (same shape as in gather)
///   - updates: Values to scatter (same shape as gather output)
/// - Returns: Updated tensor with scattered values
@differentiable(reverse, wrt: (input, updates))
public func diffScatter(
    _ input: DifferentiableTracer,
    indices: DifferentiableTracer,
    updates: DifferentiableTracer
) -> DifferentiableTracer {
    guard let builder = DifferentiableTracer.currentBuilder else {
        fatalError("No active MLIRBuilder")
    }

    let inputShape = withoutDerivative(at: input.shape)
    let indicesShape = withoutDerivative(at: indices.shape)
    let updatesShape = withoutDerivative(at: updates.shape)
    let dtype = withoutDerivative(at: input.dtype)

    let resultSSA = builder.freshSSA()
    let inputType = tensorType(shape: inputShape, dtype: dtype)
    let indicesType = tensorType(shape: indicesShape, dtype: .int32)
    let updatesType = tensorType(shape: updatesShape, dtype: dtype)
    let outputType = tensorType(shape: inputShape, dtype: dtype)

    // scatter dimension numbers (inverse of gather)
    let indicesRank = indicesShape.count
    let updateWindowDims = (indicesRank..<(indicesRank + inputShape.count - 1)).map { $0 }
    let insertedWindowDims = [0]
    let scatterDimsToOperandDims = [0]
    let indexVectorDim = indicesRank

    let scatterDimNumbers = """
update_window_dims = [\(updateWindowDims.map(String.init).joined(separator: ", "))], \
inserted_window_dims = [\(insertedWindowDims.map(String.init).joined(separator: ", "))], \
scatter_dims_to_operand_dims = [\(scatterDimsToOperandDims.map(String.init).joined(separator: ", "))], \
index_vector_dim = \(indexVectorDim)
"""

    // Use scatter with add reduction (for gradient accumulation)
    // The region is encoded in the scatter_computation attribute
    builder.addOperation(MLIROperation(
        result: resultSSA,
        opName: "stablehlo.scatter",
        operands: [input.irValue, indices.irValue, updates.irValue],
        attributes: [
            "scatter_dimension_numbers": "#stablehlo.scatter<\(scatterDimNumbers)>",
            "unique_indices": "false",
            "scatter_computation": "add",  // Use add for accumulation
            "_scatter_types": "(\(inputType), \(indicesType), \(updatesType)) -> \(outputType)"  // Type signature for MLIR generation
        ],
        resultType: outputType
    ))

    return DifferentiableTracer(irValue: resultSSA, shape: inputShape, dtype: dtype)
}

@derivative(of: diffScatter, wrt: (input, updates))
public func diffScatterVJP(
    _ input: DifferentiableTracer,
    indices: DifferentiableTracer,
    updates: DifferentiableTracer
) -> (value: DifferentiableTracer, pullback: (DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer)) {
    let output = diffScatter(input, indices: indices, updates: updates)

    func pullback(_ dy: DifferentiableTracer) -> (DifferentiableTracer, DifferentiableTracer) {
        // Gradient w.r.t. input: just dy (scatter adds to input, gradient flows through)
        let inputGrad = dy

        // Gradient w.r.t. updates: gather dy at the scattered positions
        let updatesGrad = diffGather(dy, indices: indices)

        return (inputGrad, updatesGrad)
    }

    return (output, pullback)
}
