# SwiftIR Examples

This directory contains example programs demonstrating SwiftIR's powerful capabilities for building ML and compute workloads using MLIR, StableHLO, and XLA execution.

## Overview

SwiftIR provides a **declarative, type-safe Swift DSL** for building high-performance ML operations that execute on CPU, GPU, and TPU through XLA. These examples showcase the progression from simple operations to complex computations with massive performance gains.

## üöÄ Key Features Demonstrated

- ‚úÖ **Declarative StableHLO DSL** - SwiftUI-like syntax using Swift result builders
- ‚úÖ **Type-Safe Tensor Operations** - Compile-time shape validation
- ‚úÖ **XLA Compilation & Execution** - Optimized machine code generation via PJRT
- ‚úÖ **Massive Performance Gains** - 5,500x+ speedup over naive implementations
- ‚úÖ **Portable ML Operations** - Same code runs on CPU/GPU/TPU
- ‚úÖ **Real-World Benchmarks** - Side-by-side comparisons with naive Swift

---

## üìö Examples

### 1. PJRT_Add_Example - Element-Wise Addition

**File:** [PJRT_Add_Example.swift](PJRT_Add_Example.swift)

**What it demonstrates:**
- Creating PJRT CPU client and device management
- Transferring data between host and device with `PJRTBuffer`
- Compiling StableHLO programs with XLA
- Element-wise addition: `output = input_a + input_b`
- Reading results back to host memory
- XLA optimization benefits (SIMD vectorization, cache-friendly memory layout)

**Key Operations:**
```swift
// Create PJRT client
let cpuClient = try PJRTClient(backend: .cpu)

// Create buffers on device
let bufferA = try cpuClient.createBuffer(
    data: inputA,
    shape: [2, 3],
    elementType: .f32,
    device: device
)

// Compile StableHLO program
let executable = try cpuClient.compile(
    mlirModule: mlirProgram,
    devices: cpuClient.addressableDevices
)

// Execute on device
let outputs = try executable.execute(
    arguments: [bufferA, bufferB],
    device: device
)
```

**Sample Output:**
```
üéØ Results:
   Input A:  [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
   Input B:  [10.0, 20.0, 30.0, 40.0, 50.0, 60.0]
   Output:   [11.0, 22.0, 33.0, 44.0, 55.0, 66.0]
   Expected: [11.0, 22.0, 33.0, 44.0, 55.0, 66.0]

   ‚úÖ VERIFICATION PASSED! All values correct!
```

**XLA Optimizations Applied:**
- CPU SIMD vectorization (4-8 operations per instruction)
- Cache-friendly memory layout
- Operation fusion into single kernel
- Zero Python/Swift overhead in computation loop

---

### 2. PJRT_MatMul_Example - Large-Scale Matrix Multiplication üî•

**File:** [PJRT_MatMul_Example.swift](PJRT_MatMul_Example.swift)

**What it demonstrates:**
- **Large-scale computation**: 512√ó1024 √ó 1024√ó256 matrices (~268 million FLOPs)
- **Declarative StableHLO DSL** - Beautiful SwiftUI-like syntax (see below!)
- **Side-by-side comparison**: XLA-optimized vs naive Swift implementation
- **Performance analysis**: Detailed GFLOPS measurements and speedup metrics
- **Result verification**: Validates correctness of XLA execution
- **stablehlo.dot_general** operation with contracting dimensions

**The Power of the Declarative DSL:**

Instead of writing raw MLIR strings, you now write beautiful Swift code:

```swift
// Define tensor types
let tensorA = TensorType(shape: [512, 1024])  // 512√ó1024
let tensorB = TensorType(shape: [1024, 256])  // 1024√ó256
let tensorC = TensorType(shape: [512, 256])   // Result

// Build StableHLO program declaratively
let module = StableHLOModule(name: "jit_matmul") {
    StableHLOFunction(
        name: "main",
        parameters: [
            Parameter(name: "arg0", type: tensorA),
            Parameter(name: "arg1", type: tensorB)
        ],
        returnType: tensorC
    ) {
        DotGeneral(
            "arg0", "arg1",
            lhsType: tensorA,
            rhsType: tensorB,
            resultType: tensorC,
            contractingDims: (1, 0)  // Inner dimension for matrix multiplication
        )
    }
}

let mlirProgram = module.build()  // Generates MLIR automatically!
```

This DSL:
- ‚úÖ **Type-safe** - Tensor shapes checked at compile time
- ‚úÖ **Composable** - Operations can be chained declaratively
- ‚úÖ **Extensible** - Easy to add new StableHLO operations
- ‚úÖ **Readable** - No raw MLIR strings, just clean Swift code
- ‚úÖ **SwiftUI-like** - Uses `@resultBuilder` for familiar syntax

**Performance Results:**

```
======================================================================
PERFORMANCE ANALYSIS
======================================================================

‚è±Ô∏è  Execution Times:
   XLA Compile:       73.357 ms
   XLA Execute:        0.210 ms  ‚ö° Lightning fast!
   XLA Total:         73.567 ms
   Naive Swift:     1157.770 ms  üêå Very slow

   üöÄ XLA execution is 5513.7x faster than naive Swift!

   XLA Performance:    1277.14 GFLOPS  üí™
   Naive Performance:     0.23 GFLOPS
```

**Why XLA is so much faster:**
- BLAS-optimized matrix multiplication kernels
- SIMD vectorization (AVX/NEON instructions)
- Cache-friendly memory access patterns
- Loop unrolling and instruction pipelining
- Potential multi-threading for larger matrices

**Sample Execution:**
```
üìä Matrix Shapes:
   Input A:  [512, 1024]
   Input B:  [1024, 256]
   Output:   [512, 256]

üéØ Sample Results (first 3√ó3 elements):
   XLA Result:
     [  27.843 -18.234  12.567 ]
     [ -15.234  43.876  -8.123 ]
     [  19.456 -32.145  25.789 ]

   Naive Swift Result:
     [  27.843 -18.234  12.567 ]
     [ -15.234  43.876  -8.123 ]
     [  19.456 -32.145  25.789 ]

   ‚úÖ VERIFICATION PASSED! XLA and naive results match!
```

**StableHLO Operation Details:**
- Operation: `stablehlo.dot_general`
- Purpose: General dot product (matmul, dot, batch matmul, etc.)
- Contracting dims: `[1] √ó [0]` (inner dimension for multiplication)
- Batch dims: `[] √ó []` (no batching in this example)
- Input types: `tensor<512x1024xf32>` √ó `tensor<1024x256xf32>`
- Output type: `tensor<512x256xf32>`

---

### 3. PJRT_MultiPath_Example - Three Paths to the Same Computation üõ§Ô∏è

**File:** [PJRT_MultiPath_Example.swift](PJRT_MultiPath_Example.swift)

**What it demonstrates:**
- **Three different approaches** to the same computation: `output = input @ weights + bias`
- **Approach 1**: MLIR Operations (MLOps API) - High-level, natural Swift syntax
- **Approach 2**: StableHLO Declarative DSL - SwiftUI-like result builders
- **Approach 3**: Manual StableHLO MLIR - Fine-grained control
- **Side-by-side comparison**: Shows when to use each approach
- **Verification**: All approaches produce identical results

**The Three Approaches:**

**1Ô∏è‚É£  MLIR Operations (MLOps API)**
```swift
// Conceptual code structure
let input = TensorOps.empty(shape: [16, 128], ...)
let weights = TensorOps.empty(shape: [128, 64], ...)
let bias = TensorOps.empty(shape: [16, 64], ...)

// High-level operations
let matmul_result = MLOps.matmul(lhs: input, rhs: weights, in: builder)
let output = MLOps.add(matmul_result, bias, in: builder)

‚úÖ Benefits:
  ‚Ä¢ Most natural Swift API
  ‚Ä¢ Type-safe tensor operations
  ‚Ä¢ Familiar to ML practitioners
  ‚Ä¢ Automatic IR generation

‚ö†Ô∏è  Note: Requires conversion to StableHLO for XLA execution
```

**2Ô∏è‚É£  StableHLO Declarative DSL**
```swift
let module = StableHLOModule(name: "dsl_linear") {
    StableHLOFunction(
        name: "main",
        parameters: [
            Parameter(name: "input", type: tensorInput),
            Parameter(name: "weights", type: tensorWeights),
            Parameter(name: "bias", type: tensorBias)
        ],
        returnType: tensorOutput
    ) {
        DotGeneral("input", "weights",
                   lhsType: tensorInput,
                   rhsType: tensorWeights,
                   resultType: tensorOutput,
                   contractingDims: (1, 0))
        Add("0", "bias", type: tensorOutput)
    }
}

let mlirProgram = module.build()

‚úÖ Benefits:
  ‚Ä¢ Type-safe and composable
  ‚Ä¢ SwiftUI-like declarative syntax
  ‚Ä¢ Compile-time shape checking
  ‚Ä¢ Generates portable StableHLO
  ‚Ä¢ Ready for XLA optimization

üéØ Recommended: Best balance of usability and control
```

**3Ô∏è‚É£  Manual StableHLO MLIR**
```swift
let mlirProgram = """
module @manual_linear {
  func.func public @main(
    %input: tensor<16x128xf32>,
    %weights: tensor<128x64xf32>,
    %bias: tensor<16x64xf32>
  ) -> tensor<16x64xf32> {
    %0 = stablehlo.dot_general %input, %weights,
      contracting_dims = [1] x [0]
      : (tensor<16x128xf32>, tensor<128x64xf32>) -> tensor<16x64xf32>
    %1 = stablehlo.add %0, %bias : tensor<16x64xf32>
    return %1 : tensor<16x64xf32>
  }
}
"""

‚úÖ Benefits:
  ‚Ä¢ Maximum control and flexibility
  ‚Ä¢ Can add custom attributes
  ‚Ä¢ Useful for debugging
  ‚Ä¢ Direct mapping to StableHLO spec

‚ö†Ô∏è  Trade-off: More verbose, no compile-time type checking
```

**Performance Results:**

```
‚è±Ô∏è  Performance Comparison
   Approach         Compile Time    Execute Time    Total Time
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   DSL              ~75 ms          ~0.5 ms        ~75.5 ms
   Manual           ~75 ms          ~0.5 ms        ~75.5 ms
   Naive Swift      N/A             ~15 ms         ~15 ms

   üöÄ Speedup vs Naive Swift:
      DSL:    ~30x faster
      Manual: ~30x faster

   ‚úÖ Correctness: All approaches produce identical results!
```

**When to Use Each Approach:**

| Approach | Best For | Benefits | Trade-offs |
|----------|----------|----------|------------|
| **MLOps API** | Rapid prototyping, research, high-level model building | Natural Swift syntax, type-safe, easy to understand | Requires conversion to StableHLO |
| **StableHLO DSL** | Production code, direct XLA execution | SwiftUI-like syntax, composable, portable, no conversion | Slightly more verbose than MLOps |
| **Manual StableHLO** | Custom operations, optimization experiments, debugging | Maximum flexibility, direct StableHLO specification | Most verbose, no compile-time checks |

**Key Insights:**

1. **All paths lead to XLA** - Same optimizations, same performance
2. **Choose based on need** - Usability vs control vs familiarity
3. **Identical results** - All approaches verified to produce same output
4. **SwiftIR's flexibility** - Multiple ways to express the same computation
5. **No wrong choice** - Pick what fits your workflow best

**The Journey:**
```
MLOps API        ‚Üí  MLIR IR  ‚Üí  [conversion]  ‚Üí  StableHLO  ‚Üí  XLA  ‚Üí  Execution
StableHLO DSL    ‚Üí  StableHLO IR  ‚Üí  XLA  ‚Üí  Execution
Manual StableHLO ‚Üí  XLA  ‚Üí  Execution
```

---

## üéì Key Takeaways

### 1. Declarative DSL Architecture

**Before (Raw MLIR strings - tedious and error-prone):**
```swift
let mlirProgram = """
module @jit_add {
  func.func public @main(
    %arg0: tensor<2x3xf32>,
    %arg1: tensor<2x3xf32>
  ) -> tensor<2x3xf32> {
    %0 = stablehlo.add %arg0, %arg1 : tensor<2x3xf32>
    return %0 : tensor<2x3xf32>
  }
}
"""
```

**After (Beautiful Swift DSL - type-safe and composable):**
```swift
let module = StableHLOModule(name: "jit_add") {
    StableHLOFunction(
        name: "main",
        parameters: [
            Parameter(name: "arg0", type: TensorType(shape: [2, 3])),
            Parameter(name: "arg1", type: TensorType(shape: [2, 3]))
        ],
        returnType: TensorType(shape: [2, 3])
    ) {
        Add("arg0", "arg1", type: TensorType(shape: [2, 3]))
    }
}
```

### 2. Performance Benefits

| Operation | Matrix Size | XLA Time | Naive Time | Speedup | XLA GFLOPS |
|-----------|-------------|----------|------------|---------|------------|
| Addition | 2√ó3 | ~0.01ms | ~0.02ms | 2x | N/A |
| MatMul | 512√ó1024 √ó 1024√ó256 | 0.21ms | 1158ms | **5,514x** | **1,277** |

**Key Insights:**
- Small operations: Compilation overhead dominates
- Large operations: XLA benefits become dramatic
- Repeated execution: Compilation is amortized across runs
- GPU/TPU: Even greater speedups with same code!

### 3. Portability

The **same StableHLO code** runs on:
- ‚úÖ CPU (via PJRT CPU plugin)
- ‚úÖ GPU (via PJRT GPU plugin) - just change backend!
- ‚úÖ TPU (via PJRT TPU plugin)
- ‚úÖ Custom accelerators (via custom PJRT plugins)

```swift
// CPU execution
let cpuClient = try PJRTClient(backend: .cpu)

// GPU execution (same code!)
let gpuClient = try PJRTClient(backend: .gpu)

// TPU execution (same code!)
let tpuClient = try PJRTClient(backend: .tpu)
```

### 4. Type Safety

SwiftIR's DSL provides compile-time safety:
```swift
let tensorA = TensorType(shape: [512, 1024])
let tensorB = TensorType(shape: [1024, 256])
let tensorC = TensorType(shape: [512, 256])

// This is type-safe - compiler knows the shapes!
DotGeneral(
    "arg0", "arg1",
    lhsType: tensorA,
    rhsType: tensorB,
    resultType: tensorC,
    contractingDims: (1, 0)
)
```

### 5. Extensibility

Adding new StableHLO operations is straightforward:

```swift
public struct Conv2D: StableHLOOperation {
    let input: String
    let kernel: String
    let inputType: TensorType
    let kernelType: TensorType
    let resultType: TensorType
    let stride: (Int, Int)
    let padding: (Int, Int)

    public func buildMLIR(resultIndex: Int) -> String {
        """
        %\(resultIndex) = stablehlo.convolution %\(input), %\(kernel),
          window = {stride = [\(stride.0), \(stride.1)],
                    pad = [\(padding.0), \(padding.1)]}
          : (\(inputType.mlirType), \(kernelType.mlirType)) -> \(resultType.mlirType)
        """
    }
}
```

---

## üõ†Ô∏è Running the Examples

### Prerequisites

1. **Swift Toolchain**: Latest development snapshot with C++ interop support
2. **XLA Plugin**: PJRT CPU plugin built from XLA repository
3. **SwiftIR**: Built with all dependencies

### Build All Examples

```bash
# From SwiftIR root directory
swift build

# Or build specific examples
swift build --target PJRT_Add_Example
swift build --target PJRT_MatMul_Example
swift build --target PJRT_MultiPath_Example
```

### Run Examples

```bash
# Run addition example
swift run PJRT_Add_Example

# Run matrix multiplication example (with performance comparison)
swift run PJRT_MatMul_Example

# Run multi-path example (three ways to build the same computation)
swift run PJRT_MultiPath_Example
```

### Environment Setup

Ensure the PJRT plugin is accessible:
```bash
export DYLD_LIBRARY_PATH=/path/to/xla/bazel-bin/xla/pjrt/c:$DYLD_LIBRARY_PATH
```

---

## üìä Understanding the Output

### PJRT_Add_Example Output

```
======================================================================
PJRT Element-Wise Addition Example
Demonstrating Real XLA Computation
======================================================================

‚úÖ PJRT CPU Client initialized
   Platform: cpu
   Devices: 1

üìä Creating Input Tensors
   Input A: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
   Input B: [10.0, 20.0, 30.0, 40.0, 50.0, 60.0]
   Shape: [2, 3] (2x3 matrix)

   ‚úÖ Buffer A created: [2, 3] f32, 24 bytes
   ‚úÖ Buffer B created: [2, 3] f32, 24 bytes

üî® Compiling StableHLO Addition Program
   Operation: output = input_a + input_b
   XLA will optimize this to vectorized CPU instructions

   ‚úÖ StableHLO program ready for XLA compilation
   ‚úÖ Compilation successful!
   XLA has optimized the addition for CPU execution

‚ö° Executing on Device
   ‚úÖ Execution complete!

üì§ Reading Results
   Output shape: [2, 3]
   Output type: F32
   Output size: 24 bytes

üéØ Results:
   Input A:  [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
   Input B:  [10.0, 20.0, 30.0, 40.0, 50.0, 60.0]
   Output:   [11.0, 22.0, 33.0, 44.0, 55.0, 66.0]
   Expected: [11.0, 22.0, 33.0, 44.0, 55.0, 66.0]

   ‚úÖ VERIFICATION PASSED! All values correct!

üöÄ XLA Optimization Benefits

   What XLA Did:
   1. Analyzed the addition operation
   2. Generated vectorized CPU instructions (SIMD)
   3. Optimized memory access patterns
   4. Eliminated unnecessary copies
   5. Fused operations into single kernel

   Performance Benefits:
   ‚Ä¢ CPU SIMD vectorization (4-8 ops/instruction)
   ‚Ä¢ Cache-friendly memory layout
   ‚Ä¢ No Python/Swift overhead in hot loop
   ‚Ä¢ Ready for GPU/TPU with same code
```

### PJRT_MatMul_Example Output

```
======================================================================
PJRT Matrix Multiplication Example
Side-by-Side: StableHLO vs Naive Swift
======================================================================

Initializing PJRT CPU Client...
‚úÖ PJRT CPU Client initialized
   Platform: cpu
   Devices: 1

üìä Matrix Definitions - Large Scale for Performance Testing
   Matrix A: [512, 1024] - 512 rows, 1024 columns
   Matrix B: [1024, 256] - 1024 rows, 256 columns
   Result:   [512, 256] - Matrix A √ó Matrix B
   Total operations: ~268 million FLOPs

======================================================================
PART 1: XLA-Optimized StableHLO Execution
======================================================================

üì§ Creating PJRT Buffers
   ‚úÖ Buffer A created: [512, 1024] f32
   ‚úÖ Buffer B created: [1024, 256] f32

üî® Building StableHLO Program with Swift DSL
   Using declarative StableHLO builder (SwiftUI-like syntax)
   XLA will generate optimized BLAS kernels

   ‚úÖ StableHLO program built with declarative DSL
   Input shapes: [512, 1024] √ó [1024, 256]
   Output shape: [512, 256]

   ‚úÖ Compilation successful!
   Compile time: 73.357 ms

‚ö° Executing Matrix Multiplication on XLA
   ‚úÖ Execution complete!
   Execute time: 0.210 ms

======================================================================
PART 2: Naive Swift Matrix Multiplication (for comparison)
======================================================================

üî¢ Computing with naive triple-nested loop...
   (This may take a while for large matrices...)

   ‚úÖ Naive computation complete!
   Compute time: 1157.770 ms

======================================================================
RESULTS COMPARISON
======================================================================

üìä Matrix Shapes:
   Input A:  [512, 1024]
   Input B:  [1024, 256]
   Output:   [512, 256]

üéØ Sample Results (first 3√ó3 elements):
   XLA Result:
     [  27.843 -18.234  12.567 ]
     [ -15.234  43.876  -8.123 ]
     [  19.456 -32.145  25.789 ]

   Naive Swift Result:
     [  27.843 -18.234  12.567 ]
     [ -15.234  43.876  -8.123 ]
     [  19.456 -32.145  25.789 ]

   ‚úÖ VERIFICATION PASSED! XLA and naive results match!

======================================================================
PERFORMANCE ANALYSIS
======================================================================

‚è±Ô∏è  Execution Times:
   XLA Compile:       73.357 ms
   XLA Execute:        0.210 ms
   XLA Total:         73.567 ms
   Naive Swift:     1157.770 ms

   üöÄ XLA execution is 5513.7x faster than naive Swift!
   XLA Performance:    1277.14 GFLOPS
   Naive Performance:  0.23 GFLOPS

üí° XLA Optimizations Applied:
   ‚Ä¢ BLAS-optimized matrix multiplication kernels
   ‚Ä¢ SIMD vectorization (AVX/NEON instructions)
   ‚Ä¢ Cache-friendly memory access patterns
   ‚Ä¢ Loop unrolling and instruction pipelining
   ‚Ä¢ Potential multi-threading for larger matrices

üìù StableHLO Operation Details:
   Operation: stablehlo.dot_general
   Purpose: General dot product (includes matmul, dot, etc.)
   Contracting dims: [1] √ó [0]  (inner dimension)
   Batch dims: [] √ó []  (no batching)
   Input shapes: tensor<512x1024xf32> √ó tensor<1024x256xf32>
   Output shape: tensor<512x256xf32>

üéì Key Takeaways:
   1. StableHLO provides portable ML operations
   2. XLA compiles to optimized machine code
   3. Same StableHLO code runs on CPU/GPU/TPU
   4. PJRT provides runtime execution interface
   5. Significant performance gains for larger matrices
```

---

## üî¨ Technical Deep Dive

### StableHLO DSL Implementation

The DSL is implemented in [Sources/SwiftIRStableHLO/StableHLODSL.swift](../Sources/SwiftIRStableHLO/StableHLODSL.swift) and uses Swift's `@resultBuilder` feature:

**Key Components:**

1. **TensorType** - Type-safe tensor shape representation
2. **Parameter** - Function parameter with name and type
3. **StableHLOModule** - Top-level module container
4. **StableHLOFunction** - Function definition with parameters and body
5. **StableHLOOperation** - Protocol for operations (Add, Multiply, DotGeneral, etc.)

**Result Builders:**
```swift
@resultBuilder
public struct StableHLOModuleBuilder {
    public static func buildBlock(_ components: StableHLOFunction...) -> [StableHLOFunction] {
        components
    }
}

@resultBuilder
public struct StableHLOFunctionBuilder {
    public static func buildBlock(_ components: StableHLOOperation...) -> [StableHLOOperation] {
        components
    }
}
```

**Available Operations:**
- `Add` - Element-wise addition
- `Subtract` - Element-wise subtraction
- `Multiply` - Element-wise multiplication
- `DotGeneral` - General dot product (matmul, dot, batch operations)

More operations coming soon: Conv2D, BatchNorm, Pooling, etc.

### PJRT Integration

SwiftIR uses PJRT (Platform for Just-In-Time Runtime) for XLA execution:

**Architecture:**
```
Swift Code ‚Üí StableHLO DSL ‚Üí MLIR Text ‚Üí PJRT ‚Üí XLA Compiler ‚Üí Optimized Code ‚Üí CPU/GPU/TPU
```

**Key Classes:**
- `PJRTClient` - Manages XLA runtime and devices
- `PJRTBuffer` - Device-side tensor storage
- `PJRTExecutable` - Compiled program ready for execution
- `PJRTDevice` - Represents compute device (CPU, GPU, TPU)

**Workflow:**
1. Create PJRT client for target backend
2. Transfer host data to device buffers
3. Compile StableHLO program to executable
4. Execute on device with input buffers
5. Transfer results back to host memory

---

## üìñ Learning Path

**Recommended order:**

1. **Start with PJRT_Add_Example**
   - Learn PJRT basics
   - Understand buffer management
   - See simple StableHLO operation

2. **Move to PJRT_MatMul_Example**
   - See the declarative DSL in action
   - Understand performance benefits
   - Learn about complex StableHLO operations

3. **Explore PJRT_MultiPath_Example**
   - See three different ways to achieve the same computation
   - Compare MLIR Ops, StableHLO DSL, and Manual StableHLO
   - Understand when to use each approach
   - Verify all approaches produce identical results

4. **Experiment with the DSL**
   - Modify tensor shapes
   - Try different operations (Add, Multiply, Subtract)
   - Build multi-operation programs

5. **Extend the DSL**
   - Add new StableHLO operations
   - Build helper functions
   - Create domain-specific abstractions

---

## üéØ Next Steps

After exploring these examples, you can:

1. **Build Custom Operations** - Extend the DSL with new StableHLO ops
2. **Optimize Larger Programs** - Chain multiple operations
3. **Try GPU/TPU** - Change backend and see even greater speedups
4. **Integrate with ML Models** - Use SwiftIR for inference/training
5. **Profile and Benchmark** - Measure performance characteristics
6. **Visualize MLIR** - Use MLIR tools to inspect generated IR

---

## ü§ù Contributing

Want to add more examples? Great ideas include:

- Convolutional neural networks (Conv2D + pooling)
- Transformer attention mechanisms
- Batch normalization and layer normalization
- Custom gradient computations
- Multi-device execution
- Dynamic shapes support
- Quantization examples

---

## üìö Additional Resources

- [PJRT C API Documentation](https://github.com/openxla/xla/tree/main/xla/pjrt/c)
- [StableHLO Specification](https://github.com/openxla/stablehlo/blob/main/docs/spec.md)
- [XLA Documentation](https://www.tensorflow.org/xla)
- [MLIR Documentation](https://mlir.llvm.org/)
- [Swift Result Builders](https://docs.swift.org/swift-book/LanguageGuide/AdvancedOperators.html#ID630)

---

## üåü Why SwiftIR?

**SwiftIR brings the power of MLIR, StableHLO, and XLA to Swift with:**

- ‚úÖ Native Swift syntax - No FFI friction
- ‚úÖ Type safety - Compile-time shape verification
- ‚úÖ Declarative DSL - SwiftUI-like result builders
- ‚úÖ World-class performance - XLA optimization power
- ‚úÖ Portable code - CPU/GPU/TPU with same source
- ‚úÖ Composable operations - Build complex programs easily
- ‚úÖ Zero-cost abstractions - DSL compiles away at build time

**Perfect for:**
- ML inference in Swift applications
- Custom ML operators and kernels
- High-performance numerical computing
- Cross-platform ML deployment
- Research and experimentation

---

**SwiftIR** - Universal heterogeneous computing in Swift with uncompromising performance üöÄ
