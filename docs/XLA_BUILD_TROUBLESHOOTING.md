# XLA Build Troubleshooting Guide

## Current Issue

**Error:** `xcrun: error: SDK "macosx10.11" cannot be located`

**Context:** Attempting to build PJRT CPU plugin from OpenXLA repository

## What We're Trying to Build

**Target:** `//xla/pjrt/c:pjrt_c_api_cpu_plugin.so`
**Output:** `pjrt_c_api_cpu_plugin.dylib` (macOS) or `pjrt_c_api_cpu_plugin.so` (Linux)
**Purpose:** PJRT runtime plugin for CPU execution via XLA
**Build System:** Bazel only (no CMake build available)

## Important: No CMake Build Available

**Question:** Can we build the PJRT plugin with CMake instead of Bazel?

**Answer:** **NO**. After investigating the XLA repository:
- No `CMakeLists.txt` exists in `xla/pjrt/c/` or `xla/pjrt/plugin/xla_cpu/`
- XLA uses Bazel exclusively for all builds
- The only CMake files in XLA are for the MLIR-HLO subproject (unrelated to PJRT)
- The official [PJRT Integration Guide](https://github.com/openxla/xla/blob/main/docs/pjrt/pjrt_integration.md) specifies Bazel as the build system

**Build Target:** The correct bazel target is `//xla/pjrt/c:pjrt_c_api_cpu_plugin.so`, which produces a shared library with exported symbol `GetPjrtApi`.

## Build Commands Attempted

### Attempt 1: Basic Build
```bash
cd /Users/pedro/programming/swift/xla
bazel build //xla/pjrt/c:pjrt_c_api_cpu_plugin.so
```
**Result:** âŒ Failed - `/tmp` partition out of space
**Error:** `write (No space left on device)`

### Attempt 2: Custom Build Directory
```bash
bazel --output_user_root=/Users/pedro/programming/swift/xla-build \
      build --spawn_strategy=sandboxed \
      //xla/pjrt/c:pjrt_c_api_cpu_plugin.so
```
**Result:** âŒ Failed - macOS SDK version error
**Error:** `SDK "macosx10.11" cannot be located`

### Attempt 3: With SDK Override Flags
```bash
bazel --output_user_root=/Users/pedro/programming/swift/xla-build \
      build --spawn_strategy=sandboxed \
      --macos_minimum_os=15.0 \
      --action_env=SDKROOT=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk \
      //xla/pjrt/c:pjrt_c_api_cpu_plugin.so
```
**Result:** âŒ Still failing with same SDK error
**Why:** Flags don't override cached Bazel configuration

## Root Cause Analysis

### The Problem

When Bazel first runs, it:
1. Auto-generates `@local_config_apple_cc` repository
2. Detects and caches SDK paths and versions
3. Stores this in the build directory
4. Future builds use the cached configuration

The issue is that the auto-detection chose or defaulted to **macOS 10.11**, and this is now "locked in" to the build configuration. Our command-line flags (`--macos_minimum_os`, `--action_env=SDKROOT`) are being **ignored** because they don't affect already-cached toolchain configuration.

### Where the SDK Version Comes From

1. **Auto-detection:** Bazel's `@local_config_apple_cc` repository tries to detect available SDKs
2. **Configuration files:**
   - `xla_configure.bazelrc` (generated by `configure.py`)
   - `tensorflow.bazelrc` (XLA defaults)
   - `.bazelrc` (project-wide settings)
3. **Environment variables:** `SDKROOT`, `DEVELOPER_DIR`

The macOS 10.11 SDK reference is likely coming from XLA's default configuration targeting maximum backward compatibility.

## Solution Options

### Option 1: Clean and Reconfigure âš¡ (QUICK)

**Pros:** Forces fresh SDK detection
**Cons:** May pick wrong SDK again
**Time:** 2-3 minutes + build time

```bash
cd /Users/pedro/programming/swift/xla

# Kill any running bazel
bazel shutdown

# Clean everything
bazel clean --expunge
rm -rf /Users/pedro/programming/swift/xla-build

# Reconfigure
python3 ./configure.py --backend=CPU

# Try build again
bazel --output_user_root=/Users/pedro/programming/swift/xla-build \
      build //xla/pjrt/c:pjrt_c_api_cpu_plugin.so
```

### Option 2: Override Toolchain Config ðŸŽ¯ (RECOMMENDED)

**Pros:** Explicit control over SDK
**Cons:** Requires editing config file
**Time:** 1 minute + build time

```bash
cd /Users/pedro/programming/swift/xla

# Add to xla_configure.bazelrc
cat >> xla_configure.bazelrc << 'EOF'

# Force use of current macOS SDK
build --action_env=DEVELOPER_DIR=/Library/Developer/CommandLineTools
build --action_env=SDKROOT=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk
build --macos_sdk_version=15.0
EOF

# Clean and rebuild
bazel shutdown
bazel clean --expunge
bazel --output_user_root=/Users/pedro/programming/swift/xla-build \
      build //xla/pjrt/c:pjrt_c_api_cpu_plugin.so
```

### Option 3: Use Bazelisk ðŸ“¦ (OFFICIAL)

**Pros:** Recommended by XLA team, manages bazel versions
**Cons:** Additional tool to install
**Time:** 5 minutes + build time

```bash
# Install bazelisk (bazel version manager)
brew install bazelisk

cd /Users/pedro/programming/swift/xla

# Configure
python3 build_tools/configure.py --backend=CPU

# Build using bazelisk
bazelisk build //xla/pjrt/c:pjrt_c_api_cpu_plugin.so

# Output will be at:
# bazel-bin/xla/pjrt/c/pjrt_c_api_cpu_plugin.dylib
```

### Option 4: Build on Linux ðŸ§ (MOST RELIABLE)

**Pros:** Linux builds are well-tested, no SDK issues
**Cons:** Requires Linux environment
**Time:** 15-30 minutes

**Using Docker:**
```bash
# Start Ubuntu container with volume mount
docker run -it -v /Users/pedro/programming/swift:/mnt ubuntu:22.04

# Inside container:
apt-get update
apt-get install -y git python3 python3-pip build-essential

# Install bazel
pip3 install bazelisk

# Clone and build
cd /mnt
git clone https://github.com/openxla/xla.git
cd xla
python3 ./configure.py --backend=CPU
bazelisk build //xla/pjrt/c:pjrt_c_api_cpu_plugin.so

# Output at: bazel-bin/xla/pjrt/c/pjrt_c_api_cpu_plugin.so
# Copy to: /mnt/SwiftIR/lib/pjrt_c_api_cpu_plugin.so
```

**Using Multipass (Ubuntu VM):**
```bash
# Install multipass
brew install multipass

# Launch Ubuntu VM
multipass launch --name xla-build --cpus 4 --memory 8G --disk 40G

# Enter VM
multipass shell xla-build

# Inside VM, same as Docker instructions above
```

### Option 5: Use Pre-built Libraries ðŸ“¥ (FASTEST)

**Pros:** No build required
**Cons:** May not be available for all platforms
**Time:** Instant

```bash
# Check OpenXLA releases
open https://github.com/openxla/xla/releases

# Or check TensorFlow (includes PJRT)
open https://www.tensorflow.org/install

# Or use pip-installed tensorflow's PJRT:
python3 -m pip install tensorflow
# Find library at:
# ~/miniforge3/lib/python3.11/site-packages/tensorflow/compiler/xla/pjrt/c/
```

### Option 6: Skip PJRT, Use SPIR-V Instead ðŸ”„ (ALTERNATIVE)

**Pros:** SPIR-V architecture is complete, different approach
**Cons:** Different execution path
**Time:** Depends on MLIR library availability

```bash
# Build MLIR GPU/SPIR-V dialects instead
cd /Users/pedro/programming/swift/stablehlo/llvm-build
ninja MLIRGPUDialect MLIRSPIRVDialect MLIRGPUToSPIRV

# Use SPIR-V path for GPU execution instead of PJRT
# See docs/SPIRV_TODO.md for details
```

## Recommended Path Forward

### For Immediate Testing

**Use Option 5** (Pre-built libraries from TensorFlow):
- Fastest solution
- Likely available in your conda environment
- Can test integration immediately

### For Production Use

**Use Option 4** (Linux build):
- Most reliable
- Matches production deployment targets
- Avoids macOS-specific toolchain issues

### For Development Iteration

**Use Option 2** (Override config):
- Quick to try
- Keeps everything on macOS
- Good for development workflow

## Expected Build Output

### Success Indicators

```
Target //xla/pjrt/c:pjrt_c_api_cpu_plugin.so up-to-date:
  bazel-bin/xla/pjrt/c/pjrt_c_api_cpu_plugin.dylib  (macOS)
  bazel-bin/xla/pjrt/c/pjrt_c_api_cpu_plugin.so     (Linux)

INFO: Elapsed time: 450.123s, Critical Path: 89.45s
INFO: 1234 processes: 567 internal, 667 darwin-sandbox.
INFO: Build completed successfully, 1234 total actions
```

### File Size

Expect: **~50-200 MB** (depending on build config)

### Verification

```bash
# Check symbols (macOS)
nm -gU bazel-bin/xla/pjrt/c/pjrt_c_api_cpu_plugin.dylib | grep GetPjrtApi
# Should see: _GetPjrtApi

# Or on Linux
nm -D bazel-bin/xla/pjrt/c/pjrt_c_api_cpu_plugin.so | grep GetPjrtApi
# Should see: GetPjrtApi
```

## Once Build Succeeds

### Copy to SwiftIR

```bash
# Create lib directory
mkdir -p /Users/pedro/programming/swift/SwiftIR/lib

# Copy library
cp bazel-bin/xla/pjrt/c/pjrt_c_api_cpu_plugin.dylib \
   /Users/pedro/programming/swift/SwiftIR/lib/
```

### Update Package.swift

Add linker flags (see [PJRT_INTEGRATION.md](PJRT_INTEGRATION.md)):

```swift
.target(
    name: "SwiftIRXLA",
    dependencies: ["SwiftIRCore"],
    linkerSettings: [
        .unsafeFlags([
            "-L", "/Users/pedro/programming/swift/SwiftIR/lib",
            "/Users/pedro/programming/swift/SwiftIR/lib/pjrt_c_api_cpu_plugin.dylib",
            "-Xlinker", "-rpath",
            "-Xlinker", "/Users/pedro/programming/swift/SwiftIR/lib"
        ])
    ]
)
```

### Replace Stub Implementations

Edit `Sources/SwiftIRXLA/PJRT/PJRTClient.swift`:
- Remove "stub" print statements
- Call actual PJRT C API functions
- Use dynamic loading from `PJRTWrapper.h`

### Test

```bash
swift build --target PJRT_Example
swift run PJRT_Example
```

Expected output:
```
======================================================================
PJRT Integration Example - True XLA Acceleration
======================================================================

ðŸ“¦ Stage 1: Loading PJRT Plugins
   âœ… CPU Client created
   Platform: cpu
   Devices: 1
      â€¢ CPU:0
...
```

## Alternative: SPIR-V Path

If XLA build continues to be problematic, the SPIR-V integration provides an alternative GPU execution path. See:
- [SPIRV_INTEGRATION.md](SPIRV_INTEGRATION.md) - Architecture guide
- [SPIRV_TODO.md](SPIRV_TODO.md) - Implementation roadmap

SPIR-V advantages:
- âœ… No XLA dependency
- âœ… Cross-platform (Windows/Linux/macOS/iOS/Android)
- âœ… Mobile GPU support
- âœ… Graphics integration

## Summary

**Current Status:** Bazel is using cached SDK configuration (macOS 10.11) that no longer exists on the system.

**Quick Fix:** Try Option 2 (override config) - 5 minutes
**Reliable Fix:** Try Option 4 (Linux build) - 30 minutes
**Alternative:** Use SPIR-V path instead of PJRT

**Impact:** Low urgency - PJRT integration architecture is complete. The library is needed only for actual execution, and we have working CPU execution via LLVM in the meantime.
